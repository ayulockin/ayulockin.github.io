---
title: "RAG Techniques: From Naive to Advanced"
description: "A comprehensive guide to Retrieval Augmented Generation techniques and implementation strategies"
author: "Ayush Thakur"
date: "2023-12-15"
categories: [LLMs, RAG, Machine Learning, Tutorials]
image: "https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yt5BrQ5G-9wr4pK_kx5HDQ.jpeg"
---

# RAG Techniques: From Naive to Advanced

Imagine you're demoing your company's new AI chatbot to a potential client. You ask it about their latest product, the one they've been working on for months, and what does it return? Information from two years ago about a product they don't even sell anymore. Frustrating, right?

This is a good example of what retrieval augmented generation (RAG) prevents. It gives LLMs access to content not included in their training data, either because it could not access it, did not access it, or if it was created after the LLMs training date.

In this article, we'll explore retrieval augmented generation and common RAG techniques to enhance your LLM applications.

## What is RAG?

Retrieval Augmented Generation (RAG) is a technique that combines the power of large language models with the ability to retrieve information from external sources. Rather than relying solely on the knowledge encoded in an LLM's parameters, RAG allows models to access and reference specific information from a knowledge base.

The basic RAG pipeline consists of three main components:

1. **Retriever**: Fetches relevant documents from a knowledge base
2. **Generator**: Uses the retrieved documents as context to generate responses
3. **Knowledge Base**: A collection of documents or information sources

## Basic RAG Implementation

The simplest RAG implementation follows these steps:

1. Convert your documents into embeddings and store them in a vector database
2. When a query comes in, embed it using the same model
3. Retrieve the most similar documents based on vector similarity
4. Feed the retrieved documents as context to the LLM along with the user query
5. Generate a response based on the provided context

Here's a simplified pseudocode:

```python
# Index documents
for document in documents:
    embedding = embedding_model.encode(document)
    vector_db.add(document, embedding)

# Query
query_embedding = embedding_model.encode(user_query)
similar_docs = vector_db.search(query_embedding, top_k=3)

# Generate response
context = "\n".join(similar_docs)
prompt = f"""Answer the question based on the context below:
Context: {context}
Question: {user_query}
Answer:"""

response = llm.generate(prompt)
```

## Advanced RAG Techniques

While basic RAG works well for simple use cases, there are several ways to enhance its performance:

### Query Transformation

One issue with basic RAG is that user queries might not match the exact wording in your documents, leading to poor retrieval. Query transformation addresses this by:

1. **Expanding** the query with related terms
2. **Reformulating** the query to better match document content
3. **Breaking down** complex queries into simpler sub-queries

```python
# Instead of directly using the user query
expanded_query = llm.generate(f"""
Expand the following query with additional relevant terms:
Query: {user_query}
Expanded query:""")

# Or rewrite it
rewritten_query = llm.generate(f"""
Rewrite the following query to better match document content:
Query: {user_query}
Rewritten query:""")
```

### Reranking

Reranking is one of the most effective ways to boost RAG performance. It works by:

1. Using a fast but less accurate retriever (bi-encoder) to get top-k results
2. Applying a more powerful but slower model (cross-encoder) to rerank these initial results

This gives us the best of both worlds: the speed of approximate retrieval and the accuracy of precise comparison.

```python
# First retrieval phase - fast but approximate
initial_results = vector_db.search(query_embedding, top_k=50)

# Second reranking phase - slower but more accurate
reranked_results = []
for doc in initial_results:
    score = cross_encoder_model.score(user_query, doc)
    reranked_results.append((doc, score))

# Sort by score and take top results
final_results = sorted(reranked_results, key=lambda x: x[1], reverse=True)[:10]
```

### Context Selection

Another challenge in RAG is that too much context can overwhelm the LLM. Techniques for better context selection include:

1. **LLM-based pruning**: Use a smaller model to identify and remove irrelevant chunks
2. **LLMLingua**: Compress prompts by removing low-information tokens
3. **Self-critique**: Let the LLM assess the relevance of each chunk before final generation

### Hierarchical RAG

For large document collections, hierarchical indexing can be more efficient:

1. Create a summary index with document-level embeddings
2. Maintain a separate, more granular chunk-level index
3. First search the summary index to identify relevant documents
4. Then search only the chunks from those documents

This two-step approach reduces the search space and improves retrieval efficiency.

## Modular RAG Framework

Rather than thinking of RAG as a fixed pipeline, consider it a modular framework where each component can be optimized independently:

- **Enhanced Search**: Search across various data sources simultaneously
- **RAG Fusion**: Use multiple query strategies and combine results
- **Memory Integration**: Let the LLM guide retrieval based on conversation history
- **Smart Routing**: Direct queries to the most appropriate data sources
- **Task Adapter**: Tailor RAG for specific downstream tasks

## Conclusion

RAG is a powerful technique that can significantly enhance the capabilities of LLMs by grounding them with external knowledge. By implementing advanced RAG techniques such as query transformation, reranking, and context selection, you can build more accurate, relevant, and efficient AI systems.

The field is rapidly evolving, with new approaches emerging regularly. In future articles, we'll explore adaptive RAG, multimodal RAG, and graph-based RAG systems in more detail.

---

*This article is based on my more comprehensive guide on [Weights & Biases](https://wandb.ai/site/articles/rag-techniques/).* 