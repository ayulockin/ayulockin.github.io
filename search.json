[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ayush Thakur",
    "section": "",
    "text": "Hello! I‚Äôm an engineer by choice and heart and I love solving problems. I am not married to any school of engineering and love learning about varying technologies. I am also an active learner of biology and physics.\nI have a background in Electronics and Communication Engineering. I haven‚Äôt pursued electronics in a while now in pursuit of ML but in past life have actively built autonomous robots. I feel equipped now to merge both domains of engineering to solve ‚Äúbig‚Äù problems.\nProfessionally, I am an AI Engineer at Weights and Biases (by CoreWeave). I lead our open source integration efforts primarily. These integrations make it easy for you ML practitioners to get an MLOps stack. Currently I am specializing in building LLM applications and doing model evaluations. But over the span of 5 years at W&B, I have worn multiple hats ‚Äì like leading our Kaggle initiatives, building our most popular courses, and a ton loads of reports that SEO really well. Overall, the technical efforts have contributed to a successful merger of W&B with CoreWeave and I have big plans.\nI am also a Google Developer Expert in Machine Learning and a Kaggle Master (notebook).\nWhen I‚Äôm not coding, I enjoy watching anime (Naruto is my all-time favorite!) and sharing knowledge through technical articles. I have started with 3D printing as well. Stay tuned!\nKnow more about me from here. My external articles and other work are listed here. A detailed account of my projects can be found here.\n\n    \n\n\n\n\n\n\nNew work on RAG evaluation: RAG Techniques: From Naive to Advanced - a comprehensive guide to Retrieval Augmented Generation techniques.\nNew work: LLM Evaluation Framework with systematic evaluation approaches using W&B Sweeps.\nNew work: Building MLOps pipelines for Keras integration with comprehensive monitoring and tracking capabilities.\nGave a course on RAG++: From POC to Production covering practical implementation strategies. Find the materials here.\nOpen-sourced SwAV-TF, a TensorFlow implementation of self-supervised visual representation learning.\n\nTo know more about my projects, please refer to my GitHub profile.\nApart from the work here, I try to contribute to other platforms in the form of writing and open-source contributions. Please refer here for more details."
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Ayush Thakur",
    "section": "",
    "text": "New work on RAG evaluation: RAG Techniques: From Naive to Advanced - a comprehensive guide to Retrieval Augmented Generation techniques.\nNew work: LLM Evaluation Framework with systematic evaluation approaches using W&B Sweeps.\nNew work: Building MLOps pipelines for Keras integration with comprehensive monitoring and tracking capabilities.\nGave a course on RAG++: From POC to Production covering practical implementation strategies. Find the materials here.\nOpen-sourced SwAV-TF, a TensorFlow implementation of self-supervised visual representation learning.\n\nTo know more about my projects, please refer to my GitHub profile.\nApart from the work here, I try to contribute to other platforms in the form of writing and open-source contributions. Please refer here for more details."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I currently work as aMachine Learning Engineer at Weights & Biases, focusing on building integrations, LLM applications, LLM system evaluations, educational content and more. The crux of my work is to grow our usebase and have played a crucial role in doing so. As one of the earliest hire in the growth team, I have been involved with multiple key integrations, initiatives, projects, and more.\n\n\nMachine Learning Engineer | Weights & Biases\n2020 - Present\n\nWritten multiple blog posts and tutorials on Deep Learning, MLOps, RAG, Evaluation and more. Check them out here.\nReproduced research papers and wrote about them. Check them out here.\nDevelop MLOps pipelines for popular open-source repositories/frameworks/orgs like Keras, OpenMMLab, Meta, XGBoost, and more.\nDevelop LLMOps integrations with LLM providers like OpenAI, Cohere, CrewAI, and more.\nBuilt and evaluated wandbot, a Q&A RAG based chatbot.\nCreated courses on LLM applications and MLOps.\nLead the AI team in India and lead the OS integration efforts.\nCreated and built our Kaggle Ambassador program. Became a Kaggle Notebooks Master in the process.\n\n\n\n\n\nGoogle Developer Expert in Machine Learning (TensorFlow Core) (Aug 2022 - Present)\nKaggle Notebooks Master - Created numerous popular notebooks for competitions.\n\n\n\n\n\nProgramming Languages: Python, TensorFlow, PyTorch\nML/DL Domains: LLMs, Computer Vision, MLOps, RAG systems\nTools & Platforms: Weights & Biases, Keras, HuggingFace, OpenMMLab\nAreas of Expertise:\n\nLLM application development and system evaluations\nBuilding and optimizing MLOps pipelines\nTechnical content creation\nDeep learning model implementations\n\n\n\n\n\nBachelor of Engineering in Electronics and Communication Engineering | Netaji Subhash Engineering College\n2016 - 2020\n\n\n\n\nGoogle Developer Group - Volunteer (Nov 2018 - Nov 2019)\nRobonix 2018 - Assistant Head for Line Follower competition (Feb 2018 - Mar 2018)\n\n\n\n\n\nRAG++: From POC to Production\nTraining and Fine-tuning Large Language Models (LLMs)\n\n\n\n\nI‚Äôm passionate about open-source collaboration and enjoy discussing innovative ML approaches. In my free time, I love watching anime (Naruto is my all-time favorite, One Piece is love, and Solo Leveling is üî•).\nFeel free to reach out to me on Twitter or LinkedIn for collaborations or discussions!"
  },
  {
    "objectID": "about.html#hello-im-ayush-thakur",
    "href": "about.html#hello-im-ayush-thakur",
    "title": "About Me",
    "section": "",
    "text": "I currently work as aMachine Learning Engineer at Weights & Biases, focusing on building integrations, LLM applications, LLM system evaluations, educational content and more. The crux of my work is to grow our usebase and have played a crucial role in doing so. As one of the earliest hire in the growth team, I have been involved with multiple key integrations, initiatives, projects, and more.\n\n\nMachine Learning Engineer | Weights & Biases\n2020 - Present\n\nWritten multiple blog posts and tutorials on Deep Learning, MLOps, RAG, Evaluation and more. Check them out here.\nReproduced research papers and wrote about them. Check them out here.\nDevelop MLOps pipelines for popular open-source repositories/frameworks/orgs like Keras, OpenMMLab, Meta, XGBoost, and more.\nDevelop LLMOps integrations with LLM providers like OpenAI, Cohere, CrewAI, and more.\nBuilt and evaluated wandbot, a Q&A RAG based chatbot.\nCreated courses on LLM applications and MLOps.\nLead the AI team in India and lead the OS integration efforts.\nCreated and built our Kaggle Ambassador program. Became a Kaggle Notebooks Master in the process.\n\n\n\n\n\nGoogle Developer Expert in Machine Learning (TensorFlow Core) (Aug 2022 - Present)\nKaggle Notebooks Master - Created numerous popular notebooks for competitions.\n\n\n\n\n\nProgramming Languages: Python, TensorFlow, PyTorch\nML/DL Domains: LLMs, Computer Vision, MLOps, RAG systems\nTools & Platforms: Weights & Biases, Keras, HuggingFace, OpenMMLab\nAreas of Expertise:\n\nLLM application development and system evaluations\nBuilding and optimizing MLOps pipelines\nTechnical content creation\nDeep learning model implementations\n\n\n\n\n\nBachelor of Engineering in Electronics and Communication Engineering | Netaji Subhash Engineering College\n2016 - 2020\n\n\n\n\nGoogle Developer Group - Volunteer (Nov 2018 - Nov 2019)\nRobonix 2018 - Assistant Head for Line Follower competition (Feb 2018 - Mar 2018)\n\n\n\n\n\nRAG++: From POC to Production\nTraining and Fine-tuning Large Language Models (LLMs)\n\n\n\n\nI‚Äôm passionate about open-source collaboration and enjoy discussing innovative ML approaches. In my free time, I love watching anime (Naruto is my all-time favorite, One Piece is love, and Solo Leveling is üî•).\nFeel free to reach out to me on Twitter or LinkedIn for collaborations or discussions!"
  },
  {
    "objectID": "blog/posts/rag-techniques-overview.html",
    "href": "blog/posts/rag-techniques-overview.html",
    "title": "RAG Techniques: From Naive to Advanced",
    "section": "",
    "text": "RAG Techniques: From Naive to Advanced\nContent coming soon!"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog where I share my thoughts, experiences, and tutorials on machine learning, LLMs, and MLOps. I cover topics ranging from technical deep dives to practical implementation guides, with a focus on making complex concepts accessible.\nIf you‚Äôre interested in specific topics, use the filters above to narrow down the posts by category, or use the search function to find exactly what you‚Äôre looking for.\nFeel free to share these posts and reach out with questions or feedback. I‚Äôm always happy to engage with the community!\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nThe State of LLM Evaluation\n\n\n\n\n\n\n\nllm\n\n\nevaluation\n\n\nmachine-learning\n\n\n\n\nAn overview of current approaches and challenges in LLM evaluation\n\n\n\n\n\n\nDec 12, 2023\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nRAG Techniques: From Naive to Advanced\n\n\n\n\n\n\n\nllm\n\n\nrag\n\n\ntechniques\n\n\n\n\nAn overview of Retrieval Augmented Generation (RAG) techniques for enhancing LLM responses with external knowledge\n\n\n\n\n\n\nNov 28, 2023\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/llm-evaluation.html",
    "href": "blog/posts/llm-evaluation.html",
    "title": "The State of LLM Evaluation",
    "section": "",
    "text": "The State of LLM Evaluation\nContent coming soon!\nCheck out my repository for more information on LLM evaluation approaches: LLM Evaluation Strategies"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "TensorFlow implementation of ‚ÄúUnsupervised Learning of Visual Features by Contrasting Cluster Assignments‚Äù\nThis project provides a TensorFlow implementation of the SwAV (Swapping Assignments between Views) algorithm for self-supervised visual representation learning. The implementation allows for efficient contrastive learning by contrasting cluster assignments between different views of the same image.\nTechnologies: TensorFlow, Contrastive Learning, Computer Vision\n\n\n\nExploring the ideas from ‚ÄúDeep Ensembles: A Loss Landscape Perspective‚Äù\nThis repository explores the concepts presented in the paper ‚ÄúDeep Ensembles: A Loss Landscape Perspective.‚Äù It visualizes loss landscapes to understand why deep ensembles work better than other methods for uncertainty estimation and how they relate to the geometry of loss functions.\nTechnologies: TensorFlow, Deep Learning, Uncertainty Estimation\n\n\n\nStarter pack for NeurIPS LLM Efficiency Challenge 2023\nThis repository provides a starter kit for participants in the NeurIPS LLM Efficiency Challenge 2023. It includes code and resources to help competitors optimize LLMs for better efficiency while maintaining performance.\nTechnologies: PyTorch, LLMs, Model Optimization\n\n\n\nDeep Image Inpainting using UNET-like Vanilla Autoencoder and Partial Convolution based Autoencoder\nThis project implements image inpainting techniques using two different approaches: a UNET-like Vanilla Autoencoder and a Partial Convolution based Autoencoder. It enables filling in missing or corrupted parts of images with plausible content.\nTechnologies: PyTorch, Computer Vision, Image Generation\n\n\n\nLLM Evaluation Strategies with W&B Sweeps\nA repository showcasing various LLM evaluation strategies and leveraging Weights & Biases Sweeps to optimize LLM systems. It provides practical demonstrations of how to evaluate and improve LLM performance systematically.\nTechnologies: PyTorch, Weights & Biases, LLMs, Evaluation\n\n\n\nComprehensive Article on RAG: From naive to advanced\nAn in-depth article exploring Retrieval Augmented Generation (RAG) techniques, covering basic implementations to advanced strategies. This resource helps developers understand how to enhance LLMs with external knowledge effectively.\nTechnologies: LLMs, RAG, Information Retrieval"
  },
  {
    "objectID": "projects.html#featured-projects",
    "href": "projects.html#featured-projects",
    "title": "Projects",
    "section": "",
    "text": "TensorFlow implementation of ‚ÄúUnsupervised Learning of Visual Features by Contrasting Cluster Assignments‚Äù\nThis project provides a TensorFlow implementation of the SwAV (Swapping Assignments between Views) algorithm for self-supervised visual representation learning. The implementation allows for efficient contrastive learning by contrasting cluster assignments between different views of the same image.\nTechnologies: TensorFlow, Contrastive Learning, Computer Vision\n\n\n\nExploring the ideas from ‚ÄúDeep Ensembles: A Loss Landscape Perspective‚Äù\nThis repository explores the concepts presented in the paper ‚ÄúDeep Ensembles: A Loss Landscape Perspective.‚Äù It visualizes loss landscapes to understand why deep ensembles work better than other methods for uncertainty estimation and how they relate to the geometry of loss functions.\nTechnologies: TensorFlow, Deep Learning, Uncertainty Estimation\n\n\n\nStarter pack for NeurIPS LLM Efficiency Challenge 2023\nThis repository provides a starter kit for participants in the NeurIPS LLM Efficiency Challenge 2023. It includes code and resources to help competitors optimize LLMs for better efficiency while maintaining performance.\nTechnologies: PyTorch, LLMs, Model Optimization\n\n\n\nDeep Image Inpainting using UNET-like Vanilla Autoencoder and Partial Convolution based Autoencoder\nThis project implements image inpainting techniques using two different approaches: a UNET-like Vanilla Autoencoder and a Partial Convolution based Autoencoder. It enables filling in missing or corrupted parts of images with plausible content.\nTechnologies: PyTorch, Computer Vision, Image Generation\n\n\n\nLLM Evaluation Strategies with W&B Sweeps\nA repository showcasing various LLM evaluation strategies and leveraging Weights & Biases Sweeps to optimize LLM systems. It provides practical demonstrations of how to evaluate and improve LLM performance systematically.\nTechnologies: PyTorch, Weights & Biases, LLMs, Evaluation\n\n\n\nComprehensive Article on RAG: From naive to advanced\nAn in-depth article exploring Retrieval Augmented Generation (RAG) techniques, covering basic implementations to advanced strategies. This resource helps developers understand how to enhance LLMs with external knowledge effectively.\nTechnologies: LLMs, RAG, Information Retrieval"
  },
  {
    "objectID": "projects.html#open-source-contributions",
    "href": "projects.html#open-source-contributions",
    "title": "Projects",
    "section": "Open Source Contributions",
    "text": "Open Source Contributions\nI actively contribute to various open-source projects, focusing on machine learning tools and libraries. Some of my contributions include:\n\nKeras: Implementing MLOps pipelines and enhancements\nOpenMMLab repositories: Building and optimizing workflows\nMeta repositories: Contributing to machine learning infrastructure"
  },
  {
    "objectID": "projects.html#looking-to-collaborate",
    "href": "projects.html#looking-to-collaborate",
    "title": "Projects",
    "section": "Looking to Collaborate?",
    "text": "Looking to Collaborate?\nI‚Äôm always interested in collaborating on machine learning projects, especially in computer vision (except face detection). If you have an interesting project idea or want to collaborate, feel free to reach out to me on Twitter or GitHub."
  },
  {
    "objectID": "projects/llm-eval-sweep.html",
    "href": "projects/llm-eval-sweep.html",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "GitHub Repository\n\n\n\n\nThis project demonstrates systematic approaches to evaluating Large Language Models (LLMs) using Weights & Biases Sweeps. It provides practical examples of how to set up comprehensive evaluation pipelines for different types of LLM tasks.\n\n\n\n\nQA Evaluation: Framework for evaluating question-answering performance\nMathematical Reasoning: Specialized evaluation for mathematical problem solving\nParameter Optimization: Using W&B Sweeps to find optimal prompting strategies\nVisualization: Rich visual analytics for interpreting results\n\n\n\n\nThe repository implements several evaluation strategies:\n\n\nOne of the most powerful approaches is using another LLM (typically a more capable one) to evaluate the responses of the target LLM:\ndef evaluate_with_llm_judge(question, reference_answer, model_response):\n    \"\"\"\n    Uses a judge LLM to evaluate the quality of a model response\n    compared to a reference answer.\n    \"\"\"\n    judge_prompt = f\"\"\"\n    Question: {question}\n    Reference Answer: {reference_answer}\n    Model Response: {model_response}\n    \n    Evaluate the model response on the following criteria on a scale of 1-10:\n    1. Accuracy: How factually correct is the response?\n    2. Completeness: How complete is the response?\n    3. Relevance: How relevant is the response to the question?\n    \n    For each criterion, provide a score and a brief explanation.\n    Finally, provide an overall score.\n    \"\"\"\n    \n    # Call the judge LLM\n    evaluation = judge_llm(judge_prompt)\n    \n    # Parse the scores from the evaluation\n    # Implementation depends on the structure of the judge's response\n    \n    return parsed_scores\n\n\n\nThe project uses W&B Sweeps to systematically explore different parameters:\nsweep_config = {\n    \"method\": \"grid\",\n    \"metric\": {\"name\": \"average_accuracy\", \"goal\": \"maximize\"},\n    \"parameters\": {\n        \"temperature\": {\"values\": [0.0, 0.3, 0.7, 1.0]},\n        \"prompt_strategy\": {\"values\": [\"direct\", \"cot\", \"few_shot\"]},\n        \"max_tokens\": {\"values\": [100, 200, 500]},\n    }\n}\nThis allows exploration of how different combinations of temperature, prompting strategies, and token limits affect the model‚Äôs performance.\n\n\n\n\nThe repository includes visualizations of sweep results, demonstrating how different parameters affect various performance metrics:\n\nTemperature Impact: Lower temperatures generally improved factual accuracy but sometimes at the cost of completeness\nPrompt Strategies: Chain-of-Thought prompting significantly improved mathematical reasoning tasks\nParameter Interaction: Complex interactions between parameters highlighted the importance of systematic sweeps rather than one-at-a-time optimization\n\n\n\n\nThis evaluation framework can be applied to:\n\nModel Selection: Comparing different LLMs for specific use cases\nPrompt Engineering: Finding optimal prompting strategies\nFine-tuning Decisions: Determining whether fine-tuning would benefit specific tasks\nApplication Development: Building more robust LLM-powered applications\n\n\n\n\nOngoing and planned improvements include:\n\nAdding support for more diverse evaluation tasks\nImplementing automated evaluation for creative and open-ended generation\nIntegrating human feedback into the evaluation pipeline\nExploring adaptive evaluation strategies that adjust to model strengths and weaknesses\n\n\n\n\nTo use this framework for your own LLM evaluation:\n\nClone the repository: git clone https://github.com/ayulockin/llm-eval-sweep.git\nInstall dependencies: pip install -r requirements.txt\nConfigure your W&B credentials\nRun example evaluations: python qa_full_sweeps.py\n\nFor more details, check out the GitHub repository."
  },
  {
    "objectID": "projects/llm-eval-sweep.html#project-overview",
    "href": "projects/llm-eval-sweep.html#project-overview",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "This project demonstrates systematic approaches to evaluating Large Language Models (LLMs) using Weights & Biases Sweeps. It provides practical examples of how to set up comprehensive evaluation pipelines for different types of LLM tasks."
  },
  {
    "objectID": "projects/llm-eval-sweep.html#key-features",
    "href": "projects/llm-eval-sweep.html#key-features",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "QA Evaluation: Framework for evaluating question-answering performance\nMathematical Reasoning: Specialized evaluation for mathematical problem solving\nParameter Optimization: Using W&B Sweeps to find optimal prompting strategies\nVisualization: Rich visual analytics for interpreting results"
  },
  {
    "objectID": "projects/llm-eval-sweep.html#implementation-details",
    "href": "projects/llm-eval-sweep.html#implementation-details",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "The repository implements several evaluation strategies:\n\n\nOne of the most powerful approaches is using another LLM (typically a more capable one) to evaluate the responses of the target LLM:\ndef evaluate_with_llm_judge(question, reference_answer, model_response):\n    \"\"\"\n    Uses a judge LLM to evaluate the quality of a model response\n    compared to a reference answer.\n    \"\"\"\n    judge_prompt = f\"\"\"\n    Question: {question}\n    Reference Answer: {reference_answer}\n    Model Response: {model_response}\n    \n    Evaluate the model response on the following criteria on a scale of 1-10:\n    1. Accuracy: How factually correct is the response?\n    2. Completeness: How complete is the response?\n    3. Relevance: How relevant is the response to the question?\n    \n    For each criterion, provide a score and a brief explanation.\n    Finally, provide an overall score.\n    \"\"\"\n    \n    # Call the judge LLM\n    evaluation = judge_llm(judge_prompt)\n    \n    # Parse the scores from the evaluation\n    # Implementation depends on the structure of the judge's response\n    \n    return parsed_scores\n\n\n\nThe project uses W&B Sweeps to systematically explore different parameters:\nsweep_config = {\n    \"method\": \"grid\",\n    \"metric\": {\"name\": \"average_accuracy\", \"goal\": \"maximize\"},\n    \"parameters\": {\n        \"temperature\": {\"values\": [0.0, 0.3, 0.7, 1.0]},\n        \"prompt_strategy\": {\"values\": [\"direct\", \"cot\", \"few_shot\"]},\n        \"max_tokens\": {\"values\": [100, 200, 500]},\n    }\n}\nThis allows exploration of how different combinations of temperature, prompting strategies, and token limits affect the model‚Äôs performance."
  },
  {
    "objectID": "projects/llm-eval-sweep.html#results-and-insights",
    "href": "projects/llm-eval-sweep.html#results-and-insights",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "The repository includes visualizations of sweep results, demonstrating how different parameters affect various performance metrics:\n\nTemperature Impact: Lower temperatures generally improved factual accuracy but sometimes at the cost of completeness\nPrompt Strategies: Chain-of-Thought prompting significantly improved mathematical reasoning tasks\nParameter Interaction: Complex interactions between parameters highlighted the importance of systematic sweeps rather than one-at-a-time optimization"
  },
  {
    "objectID": "projects/llm-eval-sweep.html#practical-applications",
    "href": "projects/llm-eval-sweep.html#practical-applications",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "This evaluation framework can be applied to:\n\nModel Selection: Comparing different LLMs for specific use cases\nPrompt Engineering: Finding optimal prompting strategies\nFine-tuning Decisions: Determining whether fine-tuning would benefit specific tasks\nApplication Development: Building more robust LLM-powered applications"
  },
  {
    "objectID": "projects/llm-eval-sweep.html#future-work",
    "href": "projects/llm-eval-sweep.html#future-work",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "Ongoing and planned improvements include:\n\nAdding support for more diverse evaluation tasks\nImplementing automated evaluation for creative and open-ended generation\nIntegrating human feedback into the evaluation pipeline\nExploring adaptive evaluation strategies that adjust to model strengths and weaknesses"
  },
  {
    "objectID": "projects/llm-eval-sweep.html#get-started",
    "href": "projects/llm-eval-sweep.html#get-started",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "To use this framework for your own LLM evaluation:\n\nClone the repository: git clone https://github.com/ayulockin/llm-eval-sweep.git\nInstall dependencies: pip install -r requirements.txt\nConfigure your W&B credentials\nRun example evaluations: python qa_full_sweeps.py\n\nFor more details, check out the GitHub repository."
  },
  {
    "objectID": "authoring.html",
    "href": "authoring.html",
    "title": "Authoring",
    "section": "",
    "text": "Below are the blogs, articles, and tutorials I have written on Machine Learning, Deep Learning, and MLOps. I am fortunate to collaborate with many talented individuals globally and share my knowledge with the community.\n\n\n\n\n\nHyperparameter Optimization with Weights & Biases Sweeps\nMultimodal Few-Shot Learning with Frozen Language Models\n\n\n\n\n\nIntroduction to Image Inpainting with Deep Learning (joint work with Sayak Paul)\nUnderstanding the Effectiveness of Ensembles in Deep Learning (joint work with Sayak Paul)\nUnsupervised Visual Representation Learning with SwAV (joint work with Sayak Paul)\nBenchmarking the ü§ó Transformers for Tensorflow 2\nVisualize Class Activation Maps for Image Classification Models\nReproducible Models with W&B\nKeras XLA Benchmarks (in collaboration with Soumik Rakshit and Sayak Paul)\nVisualize Models in TensorBoard with Weights & Biases\nVisualizing Model Performance with W&B\nBuilding LLM Evaluation Frameworks\nMLOps for Computer Vision Systems\n\n\n\n\n\nI regularly publish technical articles on Medium, covering various aspects of machine learning, deep learning, and MLOps:\n\nBuilding Robust LLM Applications\nThe State of LLM Evaluation\nMachine Learning System Design: A Comprehensive Approach\nGradient Amplification Attack: Circumventing Defenses Against Backdoor Attacks\nTraining with Mixed Precision in TensorFlow 2.0\n\n\n\n\nI‚Äôve created in-depth tutorials on various ML/DL topics:\n\nTensorFlow Implementation of SwAV - GitHub repository with detailed notebooks\nLLM Evaluation Strategies - Repository demonstrating systematic LLM evaluation approaches\nE2E MLOps Pipeline on GCP for Tensorflow - End-to-end MLOps on Google Cloud Platform\n\n\n\n\nI enjoy collaborating with other practitioners in the field:\n\nOpen Source Contributions: I build MLOps pipelines for popular repositories like Keras, OpenMMLab, and Meta\nTechnical Reviews: I provide technical reviews for ML/DL content published by various platforms"
  },
  {
    "objectID": "authoring.html#weights-biases",
    "href": "authoring.html#weights-biases",
    "title": "Authoring",
    "section": "",
    "text": "Hyperparameter Optimization with Weights & Biases Sweeps\nMultimodal Few-Shot Learning with Frozen Language Models\n\n\n\n\n\nIntroduction to Image Inpainting with Deep Learning (joint work with Sayak Paul)\nUnderstanding the Effectiveness of Ensembles in Deep Learning (joint work with Sayak Paul)\nUnsupervised Visual Representation Learning with SwAV (joint work with Sayak Paul)\nBenchmarking the ü§ó Transformers for Tensorflow 2\nVisualize Class Activation Maps for Image Classification Models\nReproducible Models with W&B\nKeras XLA Benchmarks (in collaboration with Soumik Rakshit and Sayak Paul)\nVisualize Models in TensorBoard with Weights & Biases\nVisualizing Model Performance with W&B\nBuilding LLM Evaluation Frameworks\nMLOps for Computer Vision Systems"
  },
  {
    "objectID": "authoring.html#medium-articles",
    "href": "authoring.html#medium-articles",
    "title": "Authoring",
    "section": "",
    "text": "I regularly publish technical articles on Medium, covering various aspects of machine learning, deep learning, and MLOps:\n\nBuilding Robust LLM Applications\nThe State of LLM Evaluation\nMachine Learning System Design: A Comprehensive Approach\nGradient Amplification Attack: Circumventing Defenses Against Backdoor Attacks\nTraining with Mixed Precision in TensorFlow 2.0"
  },
  {
    "objectID": "authoring.html#tutorials-guides",
    "href": "authoring.html#tutorials-guides",
    "title": "Authoring",
    "section": "",
    "text": "I‚Äôve created in-depth tutorials on various ML/DL topics:\n\nTensorFlow Implementation of SwAV - GitHub repository with detailed notebooks\nLLM Evaluation Strategies - Repository demonstrating systematic LLM evaluation approaches\nE2E MLOps Pipeline on GCP for Tensorflow - End-to-end MLOps on Google Cloud Platform"
  },
  {
    "objectID": "authoring.html#collaborative-work",
    "href": "authoring.html#collaborative-work",
    "title": "Authoring",
    "section": "",
    "text": "I enjoy collaborating with other practitioners in the field:\n\nOpen Source Contributions: I build MLOps pipelines for popular repositories like Keras, OpenMMLab, and Meta\nTechnical Reviews: I provide technical reviews for ML/DL content published by various platforms"
  }
]