[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m a Machine Learning Engineer at Weights & Biases, focusing on building LLM applications, LLM system evaluations, and developing MLOps pipelines for open-source repositories.\n\n\nMachine Learning Engineer | Weights & Biases\n2020 - Present\n\nDevelop MLOps pipelines for popular open-source repositories including Keras, OpenMMLab, and Meta repositories\nBuild and evaluate LLM applications and systems\nCreate technical content and educational materials on deep learning, MLOps, and LLMs\nContribute to the W&B platform’s machine learning tools and features\n\n\n\n\n\nGoogle Developer Expert in Machine Learning (TensorFlow Core) (Aug 2022 - Present)\nKaggle Notebooks Master - Created numerous popular notebooks demonstrating Weights & Biases integration\n\n\n\n\n\nProgramming Languages: Python, TensorFlow, PyTorch\nML/DL Domains: LLMs, Computer Vision, MLOps, RAG systems\nTools & Platforms: Weights & Biases, Keras, HuggingFace, OpenMMLab\nAreas of Expertise:\n\nLLM application development and system evaluations\nBuilding and optimizing MLOps pipelines\nTechnical content creation\nDeep learning model implementations\n\n\n\n\n\nBachelor of Engineering | Netaji Subhash Engineering College\n2016 - 2020\n\n\n\n\nGoogle Developer Group - Volunteer (Nov 2018 - Nov 2019)\nRobonix 2018 - Assistant Head for Line Follower competition (Feb 2018 - Mar 2018)\n\n\n\n\n\nRAG++: From POC to Production\nTraining and Fine-tuning Large Language Models (LLMs)\n\n\n\n\nI’m passionate about open-source collaboration and enjoy discussing innovative ML approaches. In my free time, I love watching anime (Naruto is my all-time favorite, One Piece is love, and Solo Leveling is 🔥).\nFeel free to reach out to me on Twitter or LinkedIn for collaborations or discussions!"
  },
  {
    "objectID": "about.html#hello-im-ayush-thakur",
    "href": "about.html#hello-im-ayush-thakur",
    "title": "About Me",
    "section": "",
    "text": "I’m a Machine Learning Engineer at Weights & Biases, focusing on building LLM applications, LLM system evaluations, and developing MLOps pipelines for open-source repositories.\n\n\nMachine Learning Engineer | Weights & Biases\n2020 - Present\n\nDevelop MLOps pipelines for popular open-source repositories including Keras, OpenMMLab, and Meta repositories\nBuild and evaluate LLM applications and systems\nCreate technical content and educational materials on deep learning, MLOps, and LLMs\nContribute to the W&B platform’s machine learning tools and features\n\n\n\n\n\nGoogle Developer Expert in Machine Learning (TensorFlow Core) (Aug 2022 - Present)\nKaggle Notebooks Master - Created numerous popular notebooks demonstrating Weights & Biases integration\n\n\n\n\n\nProgramming Languages: Python, TensorFlow, PyTorch\nML/DL Domains: LLMs, Computer Vision, MLOps, RAG systems\nTools & Platforms: Weights & Biases, Keras, HuggingFace, OpenMMLab\nAreas of Expertise:\n\nLLM application development and system evaluations\nBuilding and optimizing MLOps pipelines\nTechnical content creation\nDeep learning model implementations\n\n\n\n\n\nBachelor of Engineering | Netaji Subhash Engineering College\n2016 - 2020\n\n\n\n\nGoogle Developer Group - Volunteer (Nov 2018 - Nov 2019)\nRobonix 2018 - Assistant Head for Line Follower competition (Feb 2018 - Mar 2018)\n\n\n\n\n\nRAG++: From POC to Production\nTraining and Fine-tuning Large Language Models (LLMs)\n\n\n\n\nI’m passionate about open-source collaboration and enjoy discussing innovative ML approaches. In my free time, I love watching anime (Naruto is my all-time favorite, One Piece is love, and Solo Leveling is 🔥).\nFeel free to reach out to me on Twitter or LinkedIn for collaborations or discussions!"
  },
  {
    "objectID": "blog/posts/rag-techniques-overview.html",
    "href": "blog/posts/rag-techniques-overview.html",
    "title": "RAG Techniques: From Naive to Advanced",
    "section": "",
    "text": "Imagine you’re demoing your company’s new AI chatbot to a potential client. You ask it about their latest product, the one they’ve been working on for months, and what does it return? Information from two years ago about a product they don’t even sell anymore. Frustrating, right?\nThis is a good example of what retrieval augmented generation (RAG) prevents. It gives LLMs access to content not included in their training data, either because it could not access it, did not access it, or if it was created after the LLMs training date.\nIn this article, we’ll explore retrieval augmented generation and common RAG techniques to enhance your LLM applications.\n\n\nRetrieval Augmented Generation (RAG) is a technique that combines the power of large language models with the ability to retrieve information from external sources. Rather than relying solely on the knowledge encoded in an LLM’s parameters, RAG allows models to access and reference specific information from a knowledge base.\nThe basic RAG pipeline consists of three main components:\n\nRetriever: Fetches relevant documents from a knowledge base\nGenerator: Uses the retrieved documents as context to generate responses\nKnowledge Base: A collection of documents or information sources\n\n\n\n\nThe simplest RAG implementation follows these steps:\n\nConvert your documents into embeddings and store them in a vector database\nWhen a query comes in, embed it using the same model\nRetrieve the most similar documents based on vector similarity\nFeed the retrieved documents as context to the LLM along with the user query\nGenerate a response based on the provided context\n\nHere’s a simplified pseudocode:\n# Index documents\nfor document in documents:\n    embedding = embedding_model.encode(document)\n    vector_db.add(document, embedding)\n\n# Query\nquery_embedding = embedding_model.encode(user_query)\nsimilar_docs = vector_db.search(query_embedding, top_k=3)\n\n# Generate response\ncontext = \"\\n\".join(similar_docs)\nprompt = f\"\"\"Answer the question based on the context below:\nContext: {context}\nQuestion: {user_query}\nAnswer:\"\"\"\n\nresponse = llm.generate(prompt)\n\n\n\nWhile basic RAG works well for simple use cases, there are several ways to enhance its performance:\n\n\nOne issue with basic RAG is that user queries might not match the exact wording in your documents, leading to poor retrieval. Query transformation addresses this by:\n\nExpanding the query with related terms\nReformulating the query to better match document content\nBreaking down complex queries into simpler sub-queries\n\n# Instead of directly using the user query\nexpanded_query = llm.generate(f\"\"\"\nExpand the following query with additional relevant terms:\nQuery: {user_query}\nExpanded query:\"\"\")\n\n# Or rewrite it\nrewritten_query = llm.generate(f\"\"\"\nRewrite the following query to better match document content:\nQuery: {user_query}\nRewritten query:\"\"\")\n\n\n\nReranking is one of the most effective ways to boost RAG performance. It works by:\n\nUsing a fast but less accurate retriever (bi-encoder) to get top-k results\nApplying a more powerful but slower model (cross-encoder) to rerank these initial results\n\nThis gives us the best of both worlds: the speed of approximate retrieval and the accuracy of precise comparison.\n# First retrieval phase - fast but approximate\ninitial_results = vector_db.search(query_embedding, top_k=50)\n\n# Second reranking phase - slower but more accurate\nreranked_results = []\nfor doc in initial_results:\n    score = cross_encoder_model.score(user_query, doc)\n    reranked_results.append((doc, score))\n\n# Sort by score and take top results\nfinal_results = sorted(reranked_results, key=lambda x: x[1], reverse=True)[:10]\n\n\n\nAnother challenge in RAG is that too much context can overwhelm the LLM. Techniques for better context selection include:\n\nLLM-based pruning: Use a smaller model to identify and remove irrelevant chunks\nLLMLingua: Compress prompts by removing low-information tokens\nSelf-critique: Let the LLM assess the relevance of each chunk before final generation\n\n\n\n\nFor large document collections, hierarchical indexing can be more efficient:\n\nCreate a summary index with document-level embeddings\nMaintain a separate, more granular chunk-level index\nFirst search the summary index to identify relevant documents\nThen search only the chunks from those documents\n\nThis two-step approach reduces the search space and improves retrieval efficiency.\n\n\n\n\nRather than thinking of RAG as a fixed pipeline, consider it a modular framework where each component can be optimized independently:\n\nEnhanced Search: Search across various data sources simultaneously\nRAG Fusion: Use multiple query strategies and combine results\nMemory Integration: Let the LLM guide retrieval based on conversation history\nSmart Routing: Direct queries to the most appropriate data sources\nTask Adapter: Tailor RAG for specific downstream tasks\n\n\n\n\nRAG is a powerful technique that can significantly enhance the capabilities of LLMs by grounding them with external knowledge. By implementing advanced RAG techniques such as query transformation, reranking, and context selection, you can build more accurate, relevant, and efficient AI systems.\nThe field is rapidly evolving, with new approaches emerging regularly. In future articles, we’ll explore adaptive RAG, multimodal RAG, and graph-based RAG systems in more detail.\n\nThis article is based on my more comprehensive guide on Weights & Biases."
  },
  {
    "objectID": "blog/posts/rag-techniques-overview.html#what-is-rag",
    "href": "blog/posts/rag-techniques-overview.html#what-is-rag",
    "title": "RAG Techniques: From Naive to Advanced",
    "section": "",
    "text": "Retrieval Augmented Generation (RAG) is a technique that combines the power of large language models with the ability to retrieve information from external sources. Rather than relying solely on the knowledge encoded in an LLM’s parameters, RAG allows models to access and reference specific information from a knowledge base.\nThe basic RAG pipeline consists of three main components:\n\nRetriever: Fetches relevant documents from a knowledge base\nGenerator: Uses the retrieved documents as context to generate responses\nKnowledge Base: A collection of documents or information sources"
  },
  {
    "objectID": "blog/posts/rag-techniques-overview.html#basic-rag-implementation",
    "href": "blog/posts/rag-techniques-overview.html#basic-rag-implementation",
    "title": "RAG Techniques: From Naive to Advanced",
    "section": "",
    "text": "The simplest RAG implementation follows these steps:\n\nConvert your documents into embeddings and store them in a vector database\nWhen a query comes in, embed it using the same model\nRetrieve the most similar documents based on vector similarity\nFeed the retrieved documents as context to the LLM along with the user query\nGenerate a response based on the provided context\n\nHere’s a simplified pseudocode:\n# Index documents\nfor document in documents:\n    embedding = embedding_model.encode(document)\n    vector_db.add(document, embedding)\n\n# Query\nquery_embedding = embedding_model.encode(user_query)\nsimilar_docs = vector_db.search(query_embedding, top_k=3)\n\n# Generate response\ncontext = \"\\n\".join(similar_docs)\nprompt = f\"\"\"Answer the question based on the context below:\nContext: {context}\nQuestion: {user_query}\nAnswer:\"\"\"\n\nresponse = llm.generate(prompt)"
  },
  {
    "objectID": "blog/posts/rag-techniques-overview.html#advanced-rag-techniques",
    "href": "blog/posts/rag-techniques-overview.html#advanced-rag-techniques",
    "title": "RAG Techniques: From Naive to Advanced",
    "section": "",
    "text": "While basic RAG works well for simple use cases, there are several ways to enhance its performance:\n\n\nOne issue with basic RAG is that user queries might not match the exact wording in your documents, leading to poor retrieval. Query transformation addresses this by:\n\nExpanding the query with related terms\nReformulating the query to better match document content\nBreaking down complex queries into simpler sub-queries\n\n# Instead of directly using the user query\nexpanded_query = llm.generate(f\"\"\"\nExpand the following query with additional relevant terms:\nQuery: {user_query}\nExpanded query:\"\"\")\n\n# Or rewrite it\nrewritten_query = llm.generate(f\"\"\"\nRewrite the following query to better match document content:\nQuery: {user_query}\nRewritten query:\"\"\")\n\n\n\nReranking is one of the most effective ways to boost RAG performance. It works by:\n\nUsing a fast but less accurate retriever (bi-encoder) to get top-k results\nApplying a more powerful but slower model (cross-encoder) to rerank these initial results\n\nThis gives us the best of both worlds: the speed of approximate retrieval and the accuracy of precise comparison.\n# First retrieval phase - fast but approximate\ninitial_results = vector_db.search(query_embedding, top_k=50)\n\n# Second reranking phase - slower but more accurate\nreranked_results = []\nfor doc in initial_results:\n    score = cross_encoder_model.score(user_query, doc)\n    reranked_results.append((doc, score))\n\n# Sort by score and take top results\nfinal_results = sorted(reranked_results, key=lambda x: x[1], reverse=True)[:10]\n\n\n\nAnother challenge in RAG is that too much context can overwhelm the LLM. Techniques for better context selection include:\n\nLLM-based pruning: Use a smaller model to identify and remove irrelevant chunks\nLLMLingua: Compress prompts by removing low-information tokens\nSelf-critique: Let the LLM assess the relevance of each chunk before final generation\n\n\n\n\nFor large document collections, hierarchical indexing can be more efficient:\n\nCreate a summary index with document-level embeddings\nMaintain a separate, more granular chunk-level index\nFirst search the summary index to identify relevant documents\nThen search only the chunks from those documents\n\nThis two-step approach reduces the search space and improves retrieval efficiency."
  },
  {
    "objectID": "blog/posts/rag-techniques-overview.html#modular-rag-framework",
    "href": "blog/posts/rag-techniques-overview.html#modular-rag-framework",
    "title": "RAG Techniques: From Naive to Advanced",
    "section": "",
    "text": "Rather than thinking of RAG as a fixed pipeline, consider it a modular framework where each component can be optimized independently:\n\nEnhanced Search: Search across various data sources simultaneously\nRAG Fusion: Use multiple query strategies and combine results\nMemory Integration: Let the LLM guide retrieval based on conversation history\nSmart Routing: Direct queries to the most appropriate data sources\nTask Adapter: Tailor RAG for specific downstream tasks"
  },
  {
    "objectID": "blog/posts/rag-techniques-overview.html#conclusion",
    "href": "blog/posts/rag-techniques-overview.html#conclusion",
    "title": "RAG Techniques: From Naive to Advanced",
    "section": "",
    "text": "RAG is a powerful technique that can significantly enhance the capabilities of LLMs by grounding them with external knowledge. By implementing advanced RAG techniques such as query transformation, reranking, and context selection, you can build more accurate, relevant, and efficient AI systems.\nThe field is rapidly evolving, with new approaches emerging regularly. In future articles, we’ll explore adaptive RAG, multimodal RAG, and graph-based RAG systems in more detail.\n\nThis article is based on my more comprehensive guide on Weights & Biases."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ayush Thakur",
    "section": "",
    "text": "Google Developer Expert TensorFlow Core Kaggle Notebooks Master\n\n\nHello! I’m a Machine Learning Engineer specializing in LLM applications, model evaluation, and MLOps. My work focuses on building robust ML pipelines, evaluating LLM systems, and developing tools for the open-source community.\nAt Weights & Biases, I develop MLOps solutions for popular repositories like Keras, OpenMMLab, and Meta. I’m also a Google Developer Expert in Machine Learning and a Kaggle Notebooks Master.\nWhen I’m not coding, I enjoy watching anime (Naruto is my all-time favorite!), sharing knowledge through technical articles, and collaborating on interesting ML projects.\n\n\nGitHub | Twitter | LinkedIn | Kaggle | W&B\n\n\nRead My Blog View My Projects Talks & Courses\n\n\n\n\n\n\n\n\n\nA comprehensive guide to Retrieval Augmented Generation techniques for enhancing LLMs with external knowledge sources.\n\n\n\nA systematic approach to evaluating LLM performance using W&B Sweeps.\n\n\n\nTensorFlow implementation of self-supervised visual representation learning through contrasting cluster assignments.\n\n\n\n\n\n\n\n\nBuilding, evaluating, and optimizing LLM-based applications.\n\n\n\nImplementing retrieval-augmented generation for knowledge-intensive applications.\n\n\n\nDeveloping robust ML pipelines and infrastructure for continuous delivery.\n\n\n\nImplementing and optimizing visual recognition systems.\n\n\n\nCreating accessible educational content on ML topics.\n\n\n\nSystematically assessing ML model performance across dimensions.\n\n\n\n\n\nI’ve created comprehensive courses on: - RAG++: From POC to Production - Training and Fine-tuning Large Language Models (LLMs)\nExplore All Courses & Talks"
  },
  {
    "objectID": "index.html#machine-learning-engineer-at-weights-biases",
    "href": "index.html#machine-learning-engineer-at-weights-biases",
    "title": "Ayush Thakur",
    "section": "",
    "text": "Google Developer Expert TensorFlow Core Kaggle Notebooks Master\n\n\nHello! I’m a Machine Learning Engineer specializing in LLM applications, model evaluation, and MLOps. My work focuses on building robust ML pipelines, evaluating LLM systems, and developing tools for the open-source community.\nAt Weights & Biases, I develop MLOps solutions for popular repositories like Keras, OpenMMLab, and Meta. I’m also a Google Developer Expert in Machine Learning and a Kaggle Notebooks Master.\nWhen I’m not coding, I enjoy watching anime (Naruto is my all-time favorite!), sharing knowledge through technical articles, and collaborating on interesting ML projects.\n\n\nGitHub | Twitter | LinkedIn | Kaggle | W&B\n\n\nRead My Blog View My Projects Talks & Courses"
  },
  {
    "objectID": "index.html#featured-work",
    "href": "index.html#featured-work",
    "title": "Ayush Thakur",
    "section": "",
    "text": "A comprehensive guide to Retrieval Augmented Generation techniques for enhancing LLMs with external knowledge sources.\n\n\n\nA systematic approach to evaluating LLM performance using W&B Sweeps.\n\n\n\nTensorFlow implementation of self-supervised visual representation learning through contrasting cluster assignments."
  },
  {
    "objectID": "index.html#areas-of-expertise",
    "href": "index.html#areas-of-expertise",
    "title": "Ayush Thakur",
    "section": "",
    "text": "Building, evaluating, and optimizing LLM-based applications.\n\n\n\nImplementing retrieval-augmented generation for knowledge-intensive applications.\n\n\n\nDeveloping robust ML pipelines and infrastructure for continuous delivery.\n\n\n\nImplementing and optimizing visual recognition systems.\n\n\n\nCreating accessible educational content on ML topics.\n\n\n\nSystematically assessing ML model performance across dimensions."
  },
  {
    "objectID": "index.html#courses-education",
    "href": "index.html#courses-education",
    "title": "Ayush Thakur",
    "section": "",
    "text": "I’ve created comprehensive courses on: - RAG++: From POC to Production - Training and Fine-tuning Large Language Models (LLMs)\nExplore All Courses & Talks"
  },
  {
    "objectID": "projects/llm-eval-sweep.html",
    "href": "projects/llm-eval-sweep.html",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "GitHub Repository\n\n\n\n\nThis project demonstrates systematic approaches to evaluating Large Language Models (LLMs) using Weights & Biases Sweeps. It provides practical examples of how to set up comprehensive evaluation pipelines for different types of LLM tasks.\n\n\n\n\nQA Evaluation: Framework for evaluating question-answering performance\nMathematical Reasoning: Specialized evaluation for mathematical problem solving\nParameter Optimization: Using W&B Sweeps to find optimal prompting strategies\nVisualization: Rich visual analytics for interpreting results\n\n\n\n\nThe repository implements several evaluation strategies:\n\n\nOne of the most powerful approaches is using another LLM (typically a more capable one) to evaluate the responses of the target LLM:\ndef evaluate_with_llm_judge(question, reference_answer, model_response):\n    \"\"\"\n    Uses a judge LLM to evaluate the quality of a model response\n    compared to a reference answer.\n    \"\"\"\n    judge_prompt = f\"\"\"\n    Question: {question}\n    Reference Answer: {reference_answer}\n    Model Response: {model_response}\n    \n    Evaluate the model response on the following criteria on a scale of 1-10:\n    1. Accuracy: How factually correct is the response?\n    2. Completeness: How complete is the response?\n    3. Relevance: How relevant is the response to the question?\n    \n    For each criterion, provide a score and a brief explanation.\n    Finally, provide an overall score.\n    \"\"\"\n    \n    # Call the judge LLM\n    evaluation = judge_llm(judge_prompt)\n    \n    # Parse the scores from the evaluation\n    # Implementation depends on the structure of the judge's response\n    \n    return parsed_scores\n\n\n\nThe project uses W&B Sweeps to systematically explore different parameters:\nsweep_config = {\n    \"method\": \"grid\",\n    \"metric\": {\"name\": \"average_accuracy\", \"goal\": \"maximize\"},\n    \"parameters\": {\n        \"temperature\": {\"values\": [0.0, 0.3, 0.7, 1.0]},\n        \"prompt_strategy\": {\"values\": [\"direct\", \"cot\", \"few_shot\"]},\n        \"max_tokens\": {\"values\": [100, 200, 500]},\n    }\n}\nThis allows exploration of how different combinations of temperature, prompting strategies, and token limits affect the model’s performance.\n\n\n\n\nThe repository includes visualizations of sweep results, demonstrating how different parameters affect various performance metrics:\n\nTemperature Impact: Lower temperatures generally improved factual accuracy but sometimes at the cost of completeness\nPrompt Strategies: Chain-of-Thought prompting significantly improved mathematical reasoning tasks\nParameter Interaction: Complex interactions between parameters highlighted the importance of systematic sweeps rather than one-at-a-time optimization\n\n\n\n\nThis evaluation framework can be applied to:\n\nModel Selection: Comparing different LLMs for specific use cases\nPrompt Engineering: Finding optimal prompting strategies\nFine-tuning Decisions: Determining whether fine-tuning would benefit specific tasks\nApplication Development: Building more robust LLM-powered applications\n\n\n\n\nOngoing and planned improvements include:\n\nAdding support for more diverse evaluation tasks\nImplementing automated evaluation for creative and open-ended generation\nIntegrating human feedback into the evaluation pipeline\nExploring adaptive evaluation strategies that adjust to model strengths and weaknesses\n\n\n\n\nTo use this framework for your own LLM evaluation:\n\nClone the repository: git clone https://github.com/ayulockin/llm-eval-sweep.git\nInstall dependencies: pip install -r requirements.txt\nConfigure your W&B credentials\nRun example evaluations: python qa_full_sweeps.py\n\nFor more details, check out the GitHub repository."
  },
  {
    "objectID": "projects/llm-eval-sweep.html#project-overview",
    "href": "projects/llm-eval-sweep.html#project-overview",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "This project demonstrates systematic approaches to evaluating Large Language Models (LLMs) using Weights & Biases Sweeps. It provides practical examples of how to set up comprehensive evaluation pipelines for different types of LLM tasks."
  },
  {
    "objectID": "projects/llm-eval-sweep.html#key-features",
    "href": "projects/llm-eval-sweep.html#key-features",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "QA Evaluation: Framework for evaluating question-answering performance\nMathematical Reasoning: Specialized evaluation for mathematical problem solving\nParameter Optimization: Using W&B Sweeps to find optimal prompting strategies\nVisualization: Rich visual analytics for interpreting results"
  },
  {
    "objectID": "projects/llm-eval-sweep.html#implementation-details",
    "href": "projects/llm-eval-sweep.html#implementation-details",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "The repository implements several evaluation strategies:\n\n\nOne of the most powerful approaches is using another LLM (typically a more capable one) to evaluate the responses of the target LLM:\ndef evaluate_with_llm_judge(question, reference_answer, model_response):\n    \"\"\"\n    Uses a judge LLM to evaluate the quality of a model response\n    compared to a reference answer.\n    \"\"\"\n    judge_prompt = f\"\"\"\n    Question: {question}\n    Reference Answer: {reference_answer}\n    Model Response: {model_response}\n    \n    Evaluate the model response on the following criteria on a scale of 1-10:\n    1. Accuracy: How factually correct is the response?\n    2. Completeness: How complete is the response?\n    3. Relevance: How relevant is the response to the question?\n    \n    For each criterion, provide a score and a brief explanation.\n    Finally, provide an overall score.\n    \"\"\"\n    \n    # Call the judge LLM\n    evaluation = judge_llm(judge_prompt)\n    \n    # Parse the scores from the evaluation\n    # Implementation depends on the structure of the judge's response\n    \n    return parsed_scores\n\n\n\nThe project uses W&B Sweeps to systematically explore different parameters:\nsweep_config = {\n    \"method\": \"grid\",\n    \"metric\": {\"name\": \"average_accuracy\", \"goal\": \"maximize\"},\n    \"parameters\": {\n        \"temperature\": {\"values\": [0.0, 0.3, 0.7, 1.0]},\n        \"prompt_strategy\": {\"values\": [\"direct\", \"cot\", \"few_shot\"]},\n        \"max_tokens\": {\"values\": [100, 200, 500]},\n    }\n}\nThis allows exploration of how different combinations of temperature, prompting strategies, and token limits affect the model’s performance."
  },
  {
    "objectID": "projects/llm-eval-sweep.html#results-and-insights",
    "href": "projects/llm-eval-sweep.html#results-and-insights",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "The repository includes visualizations of sweep results, demonstrating how different parameters affect various performance metrics:\n\nTemperature Impact: Lower temperatures generally improved factual accuracy but sometimes at the cost of completeness\nPrompt Strategies: Chain-of-Thought prompting significantly improved mathematical reasoning tasks\nParameter Interaction: Complex interactions between parameters highlighted the importance of systematic sweeps rather than one-at-a-time optimization"
  },
  {
    "objectID": "projects/llm-eval-sweep.html#practical-applications",
    "href": "projects/llm-eval-sweep.html#practical-applications",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "This evaluation framework can be applied to:\n\nModel Selection: Comparing different LLMs for specific use cases\nPrompt Engineering: Finding optimal prompting strategies\nFine-tuning Decisions: Determining whether fine-tuning would benefit specific tasks\nApplication Development: Building more robust LLM-powered applications"
  },
  {
    "objectID": "projects/llm-eval-sweep.html#future-work",
    "href": "projects/llm-eval-sweep.html#future-work",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "Ongoing and planned improvements include:\n\nAdding support for more diverse evaluation tasks\nImplementing automated evaluation for creative and open-ended generation\nIntegrating human feedback into the evaluation pipeline\nExploring adaptive evaluation strategies that adjust to model strengths and weaknesses"
  },
  {
    "objectID": "projects/llm-eval-sweep.html#get-started",
    "href": "projects/llm-eval-sweep.html#get-started",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "To use this framework for your own LLM evaluation:\n\nClone the repository: git clone https://github.com/ayulockin/llm-eval-sweep.git\nInstall dependencies: pip install -r requirements.txt\nConfigure your W&B credentials\nRun example evaluations: python qa_full_sweeps.py\n\nFor more details, check out the GitHub repository."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks & Courses",
    "section": "",
    "text": "This comprehensive course covers everything you need to know about Retrieval Augmented Generation, from basic concepts to production-ready implementations. Learn how to enhance LLMs with external knowledge sources effectively.\nTopics covered: - RAG fundamentals and architecture - Advanced retrieval techniques - Context processing and optimization - Evaluation and monitoring - Deployment considerations\nCourse Link | 2023\n\n\n\nA deep dive into the world of LLM fine-tuning, teaching you how to adapt foundation models to your specific use cases. The course covers various techniques from parameter-efficient methods to full fine-tuning approaches.\nTopics covered: - Understanding LLM architectures - Fine-tuning methods (LoRA, QLoRA, full fine-tuning) - Data preparation and preprocessing - Evaluation and benchmarking - Deployment and serving\nCourse Link | 2023"
  },
  {
    "objectID": "talks.html#courses",
    "href": "talks.html#courses",
    "title": "Talks & Courses",
    "section": "",
    "text": "This comprehensive course covers everything you need to know about Retrieval Augmented Generation, from basic concepts to production-ready implementations. Learn how to enhance LLMs with external knowledge sources effectively.\nTopics covered: - RAG fundamentals and architecture - Advanced retrieval techniques - Context processing and optimization - Evaluation and monitoring - Deployment considerations\nCourse Link | 2023\n\n\n\nA deep dive into the world of LLM fine-tuning, teaching you how to adapt foundation models to your specific use cases. The course covers various techniques from parameter-efficient methods to full fine-tuning approaches.\nTopics covered: - Understanding LLM architectures - Fine-tuning methods (LoRA, QLoRA, full fine-tuning) - Data preparation and preprocessing - Evaluation and benchmarking - Deployment and serving\nCourse Link | 2023"
  },
  {
    "objectID": "talks.html#conference-talks-workshops",
    "href": "talks.html#conference-talks-workshops",
    "title": "Talks & Courses",
    "section": "Conference Talks & Workshops",
    "text": "Conference Talks & Workshops\n\n“Advanced RAG Techniques for Production Systems”\nNeurIPS Workshop | December 2023\nA comprehensive overview of production-grade RAG systems, covering advanced techniques like reranking, query transformation, and modular architectures. The talk included live demonstrations of performance improvements from each technique.\n\n\n“Evaluating and Improving LLM Performance”\nPyData Global | November 2023\nThis workshop provided attendees with a framework for systematically evaluating LLM performance across various dimensions, along with strategies for addressing common weaknesses. Participants learned how to use Weights & Biases to track experiments and improvements.\n\n\n“MLOps for Computer Vision Systems”\nTensorFlow Meetup | May 2022\nAs a Google Developer Expert in TensorFlow, I presented best practices for operationalizing computer vision models, from experiment tracking to deployment. The talk showcased how Weights & Biases can streamline the ML lifecycle."
  },
  {
    "objectID": "talks.html#tutorial-videos",
    "href": "talks.html#tutorial-videos",
    "title": "Talks & Courses",
    "section": "Tutorial Videos",
    "text": "Tutorial Videos\n\n\nUnderstanding Loss Landscapes\nAn exploration of how loss landscapes impact model performance and why ensemble methods work well.\nWatch Video | 25 minutes\n\n\nBuilding a RAG System from Scratch\nStep-by-step tutorial on implementing a basic RAG system using open-source tools.\nWatch Video | 35 minutes\n\n\nFine-tuning LLMs with LoRA\nLearn how to efficiently fine-tune large language models using Parameter-Efficient Fine-Tuning (PEFT) methods.\nWatch Video | 40 minutes"
  },
  {
    "objectID": "talks.html#upcoming-events",
    "href": "talks.html#upcoming-events",
    "title": "Talks & Courses",
    "section": "Upcoming Events",
    "text": "Upcoming Events\n\n“LLM Systems: From Prototype to Production” - ML Summit | June 2024\n“Evaluating Hallucinations in LLMs” - AI Conference | August 2024"
  },
  {
    "objectID": "talks.html#interested-in-having-me-speak",
    "href": "talks.html#interested-in-having-me-speak",
    "title": "Talks & Courses",
    "section": "Interested in Having Me Speak?",
    "text": "Interested in Having Me Speak?\nIf you’re organizing a conference, workshop, or meetup related to machine learning, LLMs, or MLOps, I’d be happy to discuss speaking opportunities. Please contact me with details about your event."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "TensorFlow implementation of “Unsupervised Learning of Visual Features by Contrasting Cluster Assignments”\nThis project provides a TensorFlow implementation of the SwAV (Swapping Assignments between Views) algorithm for self-supervised visual representation learning. The implementation allows for efficient contrastive learning by contrasting cluster assignments between different views of the same image.\nTechnologies: TensorFlow, Contrastive Learning, Computer Vision\n\n\n\nExploring the ideas from “Deep Ensembles: A Loss Landscape Perspective”\nThis repository explores the concepts presented in the paper “Deep Ensembles: A Loss Landscape Perspective.” It visualizes loss landscapes to understand why deep ensembles work better than other methods for uncertainty estimation and how they relate to the geometry of loss functions.\nTechnologies: TensorFlow, Deep Learning, Uncertainty Estimation\n\n\n\nStarter pack for NeurIPS LLM Efficiency Challenge 2023\nThis repository provides a starter kit for participants in the NeurIPS LLM Efficiency Challenge 2023. It includes code and resources to help competitors optimize LLMs for better efficiency while maintaining performance.\nTechnologies: PyTorch, LLMs, Model Optimization\n\n\n\nDeep Image Inpainting using UNET-like Vanilla Autoencoder and Partial Convolution based Autoencoder\nThis project implements image inpainting techniques using two different approaches: a UNET-like Vanilla Autoencoder and a Partial Convolution based Autoencoder. It enables filling in missing or corrupted parts of images with plausible content.\nTechnologies: PyTorch, Computer Vision, Image Generation\n\n\n\nLLM Evaluation Strategies with W&B Sweeps\nA repository showcasing various LLM evaluation strategies and leveraging Weights & Biases Sweeps to optimize LLM systems. It provides practical demonstrations of how to evaluate and improve LLM performance systematically.\nTechnologies: PyTorch, Weights & Biases, LLMs, Evaluation\n\n\n\nComprehensive Article on RAG: From naive to advanced\nAn in-depth article exploring Retrieval Augmented Generation (RAG) techniques, covering basic implementations to advanced strategies. This resource helps developers understand how to enhance LLMs with external knowledge effectively.\nTechnologies: LLMs, RAG, Information Retrieval"
  },
  {
    "objectID": "projects.html#featured-projects",
    "href": "projects.html#featured-projects",
    "title": "Projects",
    "section": "",
    "text": "TensorFlow implementation of “Unsupervised Learning of Visual Features by Contrasting Cluster Assignments”\nThis project provides a TensorFlow implementation of the SwAV (Swapping Assignments between Views) algorithm for self-supervised visual representation learning. The implementation allows for efficient contrastive learning by contrasting cluster assignments between different views of the same image.\nTechnologies: TensorFlow, Contrastive Learning, Computer Vision\n\n\n\nExploring the ideas from “Deep Ensembles: A Loss Landscape Perspective”\nThis repository explores the concepts presented in the paper “Deep Ensembles: A Loss Landscape Perspective.” It visualizes loss landscapes to understand why deep ensembles work better than other methods for uncertainty estimation and how they relate to the geometry of loss functions.\nTechnologies: TensorFlow, Deep Learning, Uncertainty Estimation\n\n\n\nStarter pack for NeurIPS LLM Efficiency Challenge 2023\nThis repository provides a starter kit for participants in the NeurIPS LLM Efficiency Challenge 2023. It includes code and resources to help competitors optimize LLMs for better efficiency while maintaining performance.\nTechnologies: PyTorch, LLMs, Model Optimization\n\n\n\nDeep Image Inpainting using UNET-like Vanilla Autoencoder and Partial Convolution based Autoencoder\nThis project implements image inpainting techniques using two different approaches: a UNET-like Vanilla Autoencoder and a Partial Convolution based Autoencoder. It enables filling in missing or corrupted parts of images with plausible content.\nTechnologies: PyTorch, Computer Vision, Image Generation\n\n\n\nLLM Evaluation Strategies with W&B Sweeps\nA repository showcasing various LLM evaluation strategies and leveraging Weights & Biases Sweeps to optimize LLM systems. It provides practical demonstrations of how to evaluate and improve LLM performance systematically.\nTechnologies: PyTorch, Weights & Biases, LLMs, Evaluation\n\n\n\nComprehensive Article on RAG: From naive to advanced\nAn in-depth article exploring Retrieval Augmented Generation (RAG) techniques, covering basic implementations to advanced strategies. This resource helps developers understand how to enhance LLMs with external knowledge effectively.\nTechnologies: LLMs, RAG, Information Retrieval"
  },
  {
    "objectID": "projects.html#open-source-contributions",
    "href": "projects.html#open-source-contributions",
    "title": "Projects",
    "section": "Open Source Contributions",
    "text": "Open Source Contributions\nI actively contribute to various open-source projects, focusing on machine learning tools and libraries. Some of my contributions include:\n\nKeras: Implementing MLOps pipelines and enhancements\nOpenMMLab repositories: Building and optimizing workflows\nMeta repositories: Contributing to machine learning infrastructure"
  },
  {
    "objectID": "projects.html#looking-to-collaborate",
    "href": "projects.html#looking-to-collaborate",
    "title": "Projects",
    "section": "Looking to Collaborate?",
    "text": "Looking to Collaborate?\nI’m always interested in collaborating on machine learning projects, especially in computer vision (except face detection). If you have an interesting project idea or want to collaborate, feel free to reach out to me on Twitter or GitHub."
  },
  {
    "objectID": "blog/posts/llm-evaluation.html",
    "href": "blog/posts/llm-evaluation.html",
    "title": "A Practical Guide to LLM Evaluation",
    "section": "",
    "text": "As Large Language Models (LLMs) become increasingly central to AI applications, robust evaluation becomes essential. It’s not enough to simply deploy an LLM and hope for the best – we need systematic ways to assess their performance across multiple dimensions.\nIn this article, I’ll share a practical approach to LLM evaluation, drawing from my experience building evaluation pipelines with Weights & Biases Sweeps.\n\n\nUnlike traditional ML models where metrics like accuracy or F1 score might suffice, LLMs present unique evaluation challenges:\n\nMultiple dimensions of quality: From factual accuracy to coherence to safety\nContext sensitivity: Performance can vary dramatically across different topics and scenarios\nSubjectivity: Many aspects of LLM outputs require human judgment\nRapid evolution: Evaluation frameworks need to keep pace with rapidly advancing capabilities\n\n\n\n\nA comprehensive LLM evaluation framework should include:\n\n\nSome important dimensions to consider:\n\nFactual accuracy: Does the LLM provide correct information?\nRelevance: Does the response address the query appropriately?\nCoherence: Is the response logically structured and consistent?\nConciseness: Does the response avoid unnecessary verbosity?\nSafety: Does the model avoid harmful, unethical, or biased outputs?\nFormat adherence: Does the output follow requested formats?\n\n\n\n\nFor effective evaluation, you need appropriate datasets:\n\nBenchmark datasets: Standard datasets like MMLU, HumanEval, etc., for comparative evaluation\nDomain-specific datasets: Tailored to your application area\nAdversarial examples: Test cases designed to probe specific weaknesses\nReal-world examples: Actual queries from your target use case\n\n\n\n\nDifferent aspects of LLM performance require different evaluation strategies:\n\n\n\nReference-based metrics: Compare against ground truth using metrics like BLEU, ROUGE, etc.\nModel-based evaluation: Use another LLM (often a more powerful one) to evaluate outputs\nRule-based checkers: Verify specific constraints (e.g., format validation)\n\n\n\n\n\nExpert review: Domain experts assess specific aspects\nComparative ranking: Side-by-side comparison of different models\nUser feedback: Real-world user reactions and preferences\n\n\n\n\n\n\nLet’s look at how we can implement systematic LLM evaluation using W&B Sweeps.\n\n\nHere’s a simplified approach from my llm-eval-sweep repository:\nimport wandb\nfrom wandb.sdk.data_types import WeightsTable\n\n# Initialize W&B run\nrun = wandb.init(project=\"llm-evaluation\")\n\n# Define a simple dataset for QA evaluation\nqa_pairs = [\n    {\"question\": \"What is the capital of France?\", \"reference\": \"Paris\"},\n    {\"question\": \"Who wrote Romeo and Juliet?\", \"reference\": \"William Shakespeare\"},\n    # More examples...\n]\n\n# Define LLM function (could be OpenAI, Anthropic, or any other provider)\ndef get_llm_response(question, temperature, max_tokens):\n    # Implementation depends on your LLM provider\n    return \"Sample response\"\n\n# Track model outputs and evaluations\neval_table = wandb.Table(columns=[\"question\", \"reference\", \"response\", \n                                 \"accuracy_score\", \"relevance_score\"])\n\n# Calculate scores using another LLM as judge\ndef evaluate_response(question, reference, response):\n    # Implementation of LLM-as-judge\n    return {\"accuracy\": 0.85, \"relevance\": 0.9}\n\n# Run evaluation for each example\nfor qa in qa_pairs:\n    response = get_llm_response(\n        qa[\"question\"], \n        temperature=0.7, \n        max_tokens=100\n    )\n    \n    scores = evaluate_response(qa[\"question\"], qa[\"reference\"], response)\n    \n    eval_table.add_data(\n        qa[\"question\"],\n        qa[\"reference\"],\n        response,\n        scores[\"accuracy\"],\n        scores[\"relevance\"]\n    )\n\n# Log the evaluation results\nwandb.log({\"evaluations\": eval_table})\n\n\n\nThe real power comes when we use Sweeps to systematically explore different prompt strategies and model parameters:\nsweep_configuration = {\n    \"method\": \"grid\",\n    \"parameters\": {\n        \"temperature\": {\"values\": [0.0, 0.3, 0.7, 1.0]},\n        \"prompt_template\": {\n            \"values\": [\n                \"Answer the following question: {question}\",\n                \"Given the question '{question}', provide a concise answer.\",\n                \"Answer this question accurately and briefly: {question}\"\n            ]\n        },\n        \"max_tokens\": {\"values\": [50, 100, 200]}\n    }\n}\n\nsweep_id = wandb.sweep(sweep_configuration, project=\"llm-evaluation\")\n\ndef sweep_function():\n    run = wandb.init()\n    \n    # Get sweep parameters\n    temperature = wandb.config.temperature\n    prompt_template = wandb.config.prompt_template\n    max_tokens = wandb.config.max_tokens\n    \n    # Run evaluation with these parameters\n    # ... similar to previous code, but using sweep parameters\n    \nwandb.agent(sweep_id, function=sweep_function)\nThis approach allows us to systematically explore the impact of different parameters on model performance, and visualize the results in the W&B UI.\n\n\n\n\nOne interesting application of this approach is evaluating LLM performance on mathematical problems.\nFor mathematical problems, we might define specialized evaluation dimensions:\n\nCorrectness of final answer\nValidity of the solution approach\nStep-by-step reasoning quality\n\nHere’s how we might adapt our evaluation framework:\nmath_problems = [\n    {\n        \"question\": \"Solve for x: 2x + 5 = 15\",\n        \"reference_answer\": \"x = 5\",\n        \"reference_solution\": \"2x + 5 = 15\\n2x = 10\\nx = 5\"\n    },\n    # More problems...\n]\n\n# Define specialized math evaluation\ndef evaluate_math_solution(problem, reference_answer, reference_solution, response):\n    # Extract final answer from the response\n    # Analyze solution approach\n    # Score step-by-step reasoning\n    return {\n        \"answer_correctness\": 1.0,  # Binary or graded\n        \"approach_validity\": 0.9,   # How valid was the solution approach\n        \"reasoning_quality\": 0.8    # Quality of intermediate steps\n    }\n\n\n\nAfter running our evaluations, W&B gives us powerful visualization capabilities:\n\nParallel coordinates plots: See how different parameters affect various metrics\nScatter plots: Identify relationships between different evaluation dimensions\nTables with rich media: Examine individual examples with detailed scoring\n\nThese visualizations help identify:\n\nPatterns of failure: Common scenarios where the model struggles\nParameter sensitivity: How temperature, prompt structure, etc. affect performance\nTrade-offs: Where improving one metric might harm another\n\n\n\n\nEffective LLM evaluation requires a systematic approach that addresses multiple dimensions of performance. By using tools like W&B Sweeps, we can:\n\nDefine clear evaluation criteria\nTest across diverse examples\nSystematically explore different parameters\nVisualize and analyze results\n\nThis approach not only helps in selecting the right models and parameters but also in identifying specific weaknesses that need addressing.\nRemember that evaluation is not a one-time activity but an ongoing process. As your use cases evolve and models improve, your evaluation framework should evolve too.\nIf you’re interested in exploring this topic further, check out my llm-eval-sweep repository for more examples and code samples.\n\nWhat aspects of LLM evaluation do you find most challenging? I’d love to hear about your experiences in the comments!"
  },
  {
    "objectID": "blog/posts/llm-evaluation.html#why-llm-evaluation-is-challenging",
    "href": "blog/posts/llm-evaluation.html#why-llm-evaluation-is-challenging",
    "title": "A Practical Guide to LLM Evaluation",
    "section": "",
    "text": "Unlike traditional ML models where metrics like accuracy or F1 score might suffice, LLMs present unique evaluation challenges:\n\nMultiple dimensions of quality: From factual accuracy to coherence to safety\nContext sensitivity: Performance can vary dramatically across different topics and scenarios\nSubjectivity: Many aspects of LLM outputs require human judgment\nRapid evolution: Evaluation frameworks need to keep pace with rapidly advancing capabilities"
  },
  {
    "objectID": "blog/posts/llm-evaluation.html#building-a-systematic-evaluation-framework",
    "href": "blog/posts/llm-evaluation.html#building-a-systematic-evaluation-framework",
    "title": "A Practical Guide to LLM Evaluation",
    "section": "",
    "text": "A comprehensive LLM evaluation framework should include:\n\n\nSome important dimensions to consider:\n\nFactual accuracy: Does the LLM provide correct information?\nRelevance: Does the response address the query appropriately?\nCoherence: Is the response logically structured and consistent?\nConciseness: Does the response avoid unnecessary verbosity?\nSafety: Does the model avoid harmful, unethical, or biased outputs?\nFormat adherence: Does the output follow requested formats?\n\n\n\n\nFor effective evaluation, you need appropriate datasets:\n\nBenchmark datasets: Standard datasets like MMLU, HumanEval, etc., for comparative evaluation\nDomain-specific datasets: Tailored to your application area\nAdversarial examples: Test cases designed to probe specific weaknesses\nReal-world examples: Actual queries from your target use case\n\n\n\n\nDifferent aspects of LLM performance require different evaluation strategies:\n\n\n\nReference-based metrics: Compare against ground truth using metrics like BLEU, ROUGE, etc.\nModel-based evaluation: Use another LLM (often a more powerful one) to evaluate outputs\nRule-based checkers: Verify specific constraints (e.g., format validation)\n\n\n\n\n\nExpert review: Domain experts assess specific aspects\nComparative ranking: Side-by-side comparison of different models\nUser feedback: Real-world user reactions and preferences"
  },
  {
    "objectID": "blog/posts/llm-evaluation.html#implementing-evaluation-with-weights-biases-sweeps",
    "href": "blog/posts/llm-evaluation.html#implementing-evaluation-with-weights-biases-sweeps",
    "title": "A Practical Guide to LLM Evaluation",
    "section": "",
    "text": "Let’s look at how we can implement systematic LLM evaluation using W&B Sweeps.\n\n\nHere’s a simplified approach from my llm-eval-sweep repository:\nimport wandb\nfrom wandb.sdk.data_types import WeightsTable\n\n# Initialize W&B run\nrun = wandb.init(project=\"llm-evaluation\")\n\n# Define a simple dataset for QA evaluation\nqa_pairs = [\n    {\"question\": \"What is the capital of France?\", \"reference\": \"Paris\"},\n    {\"question\": \"Who wrote Romeo and Juliet?\", \"reference\": \"William Shakespeare\"},\n    # More examples...\n]\n\n# Define LLM function (could be OpenAI, Anthropic, or any other provider)\ndef get_llm_response(question, temperature, max_tokens):\n    # Implementation depends on your LLM provider\n    return \"Sample response\"\n\n# Track model outputs and evaluations\neval_table = wandb.Table(columns=[\"question\", \"reference\", \"response\", \n                                 \"accuracy_score\", \"relevance_score\"])\n\n# Calculate scores using another LLM as judge\ndef evaluate_response(question, reference, response):\n    # Implementation of LLM-as-judge\n    return {\"accuracy\": 0.85, \"relevance\": 0.9}\n\n# Run evaluation for each example\nfor qa in qa_pairs:\n    response = get_llm_response(\n        qa[\"question\"], \n        temperature=0.7, \n        max_tokens=100\n    )\n    \n    scores = evaluate_response(qa[\"question\"], qa[\"reference\"], response)\n    \n    eval_table.add_data(\n        qa[\"question\"],\n        qa[\"reference\"],\n        response,\n        scores[\"accuracy\"],\n        scores[\"relevance\"]\n    )\n\n# Log the evaluation results\nwandb.log({\"evaluations\": eval_table})\n\n\n\nThe real power comes when we use Sweeps to systematically explore different prompt strategies and model parameters:\nsweep_configuration = {\n    \"method\": \"grid\",\n    \"parameters\": {\n        \"temperature\": {\"values\": [0.0, 0.3, 0.7, 1.0]},\n        \"prompt_template\": {\n            \"values\": [\n                \"Answer the following question: {question}\",\n                \"Given the question '{question}', provide a concise answer.\",\n                \"Answer this question accurately and briefly: {question}\"\n            ]\n        },\n        \"max_tokens\": {\"values\": [50, 100, 200]}\n    }\n}\n\nsweep_id = wandb.sweep(sweep_configuration, project=\"llm-evaluation\")\n\ndef sweep_function():\n    run = wandb.init()\n    \n    # Get sweep parameters\n    temperature = wandb.config.temperature\n    prompt_template = wandb.config.prompt_template\n    max_tokens = wandb.config.max_tokens\n    \n    # Run evaluation with these parameters\n    # ... similar to previous code, but using sweep parameters\n    \nwandb.agent(sweep_id, function=sweep_function)\nThis approach allows us to systematically explore the impact of different parameters on model performance, and visualize the results in the W&B UI."
  },
  {
    "objectID": "blog/posts/llm-evaluation.html#case-study-math-problem-evaluation",
    "href": "blog/posts/llm-evaluation.html#case-study-math-problem-evaluation",
    "title": "A Practical Guide to LLM Evaluation",
    "section": "",
    "text": "One interesting application of this approach is evaluating LLM performance on mathematical problems.\nFor mathematical problems, we might define specialized evaluation dimensions:\n\nCorrectness of final answer\nValidity of the solution approach\nStep-by-step reasoning quality\n\nHere’s how we might adapt our evaluation framework:\nmath_problems = [\n    {\n        \"question\": \"Solve for x: 2x + 5 = 15\",\n        \"reference_answer\": \"x = 5\",\n        \"reference_solution\": \"2x + 5 = 15\\n2x = 10\\nx = 5\"\n    },\n    # More problems...\n]\n\n# Define specialized math evaluation\ndef evaluate_math_solution(problem, reference_answer, reference_solution, response):\n    # Extract final answer from the response\n    # Analyze solution approach\n    # Score step-by-step reasoning\n    return {\n        \"answer_correctness\": 1.0,  # Binary or graded\n        \"approach_validity\": 0.9,   # How valid was the solution approach\n        \"reasoning_quality\": 0.8    # Quality of intermediate steps\n    }"
  },
  {
    "objectID": "blog/posts/llm-evaluation.html#visualizing-and-interpreting-results",
    "href": "blog/posts/llm-evaluation.html#visualizing-and-interpreting-results",
    "title": "A Practical Guide to LLM Evaluation",
    "section": "",
    "text": "After running our evaluations, W&B gives us powerful visualization capabilities:\n\nParallel coordinates plots: See how different parameters affect various metrics\nScatter plots: Identify relationships between different evaluation dimensions\nTables with rich media: Examine individual examples with detailed scoring\n\nThese visualizations help identify:\n\nPatterns of failure: Common scenarios where the model struggles\nParameter sensitivity: How temperature, prompt structure, etc. affect performance\nTrade-offs: Where improving one metric might harm another"
  },
  {
    "objectID": "blog/posts/llm-evaluation.html#conclusion",
    "href": "blog/posts/llm-evaluation.html#conclusion",
    "title": "A Practical Guide to LLM Evaluation",
    "section": "",
    "text": "Effective LLM evaluation requires a systematic approach that addresses multiple dimensions of performance. By using tools like W&B Sweeps, we can:\n\nDefine clear evaluation criteria\nTest across diverse examples\nSystematically explore different parameters\nVisualize and analyze results\n\nThis approach not only helps in selecting the right models and parameters but also in identifying specific weaknesses that need addressing.\nRemember that evaluation is not a one-time activity but an ongoing process. As your use cases evolve and models improve, your evaluation framework should evolve too.\nIf you’re interested in exploring this topic further, check out my llm-eval-sweep repository for more examples and code samples.\n\nWhat aspects of LLM evaluation do you find most challenging? I’d love to hear about your experiences in the comments!"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog where I share my thoughts, experiences, and tutorials on machine learning, LLMs, and MLOps. I cover topics ranging from technical deep dives to practical implementation guides, with a focus on making complex concepts accessible.\nIf you’re interested in specific topics, use the filters above to narrow down the posts by category, or use the search function to find exactly what you’re looking for.\nFeel free to share these posts and reach out with questions or feedback. I’m always happy to engage with the community!\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nA Practical Guide to LLM Evaluation\n\n\n\n\n\n\nLLMs\n\n\nEvaluation\n\n\nMLOps\n\n\nTutorials\n\n\n\nLearn how to systematically evaluate LLM performance using Weights & Biases Sweeps\n\n\n\n\n\nFeb 20, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nRAG Techniques: From Naive to Advanced\n\n\n\n\n\n\nLLMs\n\n\nRAG\n\n\nMachine Learning\n\n\nTutorials\n\n\n\nA comprehensive guide to Retrieval Augmented Generation techniques and implementation strategies\n\n\n\n\n\nDec 15, 2023\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  }
]