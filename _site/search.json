[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m a Machine Learning Engineer at Weights & Biases, focusing on building LLM applications, LLM system evaluations, and developing MLOps pipelines for open-source repositories.\n\n\nMachine Learning Engineer | Weights & Biases\n2020 - Present\n\nDevelop MLOps pipelines for popular open-source repositories including Keras, OpenMMLab, and Meta repositories\nBuild and evaluate LLM applications and systems\nCreate technical content and educational materials on deep learning, MLOps, and LLMs\nContribute to the W&B platform’s machine learning tools and features\n\n\n\n\n\nGoogle Developer Expert in Machine Learning (TensorFlow Core) (Aug 2022 - Present)\nKaggle Notebooks Master - Created numerous popular notebooks demonstrating Weights & Biases integration\n\n\n\n\n\nProgramming Languages: Python, TensorFlow, PyTorch\nML/DL Domains: LLMs, Computer Vision, MLOps, RAG systems\nTools & Platforms: Weights & Biases, Keras, HuggingFace, OpenMMLab\nAreas of Expertise:\n\nLLM application development and system evaluations\nBuilding and optimizing MLOps pipelines\nTechnical content creation\nDeep learning model implementations\n\n\n\n\n\nBachelor of Engineering | Netaji Subhash Engineering College\n2016 - 2020\n\n\n\n\nGoogle Developer Group - Volunteer (Nov 2018 - Nov 2019)\nRobonix 2018 - Assistant Head for Line Follower competition (Feb 2018 - Mar 2018)\n\n\n\n\n\nRAG++: From POC to Production\nTraining and Fine-tuning Large Language Models (LLMs)\n\n\n\n\nI’m passionate about open-source collaboration and enjoy discussing innovative ML approaches. In my free time, I love watching anime (Naruto is my all-time favorite, One Piece is love, and Solo Leveling is 🔥).\nFeel free to reach out to me on Twitter or LinkedIn for collaborations or discussions!"
  },
  {
    "objectID": "about.html#hello-im-ayush-thakur",
    "href": "about.html#hello-im-ayush-thakur",
    "title": "About Me",
    "section": "",
    "text": "I’m a Machine Learning Engineer at Weights & Biases, focusing on building LLM applications, LLM system evaluations, and developing MLOps pipelines for open-source repositories.\n\n\nMachine Learning Engineer | Weights & Biases\n2020 - Present\n\nDevelop MLOps pipelines for popular open-source repositories including Keras, OpenMMLab, and Meta repositories\nBuild and evaluate LLM applications and systems\nCreate technical content and educational materials on deep learning, MLOps, and LLMs\nContribute to the W&B platform’s machine learning tools and features\n\n\n\n\n\nGoogle Developer Expert in Machine Learning (TensorFlow Core) (Aug 2022 - Present)\nKaggle Notebooks Master - Created numerous popular notebooks demonstrating Weights & Biases integration\n\n\n\n\n\nProgramming Languages: Python, TensorFlow, PyTorch\nML/DL Domains: LLMs, Computer Vision, MLOps, RAG systems\nTools & Platforms: Weights & Biases, Keras, HuggingFace, OpenMMLab\nAreas of Expertise:\n\nLLM application development and system evaluations\nBuilding and optimizing MLOps pipelines\nTechnical content creation\nDeep learning model implementations\n\n\n\n\n\nBachelor of Engineering | Netaji Subhash Engineering College\n2016 - 2020\n\n\n\n\nGoogle Developer Group - Volunteer (Nov 2018 - Nov 2019)\nRobonix 2018 - Assistant Head for Line Follower competition (Feb 2018 - Mar 2018)\n\n\n\n\n\nRAG++: From POC to Production\nTraining and Fine-tuning Large Language Models (LLMs)\n\n\n\n\nI’m passionate about open-source collaboration and enjoy discussing innovative ML approaches. In my free time, I love watching anime (Naruto is my all-time favorite, One Piece is love, and Solo Leveling is 🔥).\nFeel free to reach out to me on Twitter or LinkedIn for collaborations or discussions!"
  },
  {
    "objectID": "blog/posts/rag-techniques-overview.html",
    "href": "blog/posts/rag-techniques-overview.html",
    "title": "RAG Techniques: From Naive to Advanced",
    "section": "",
    "text": "RAG Techniques: From Naive to Advanced\nContent coming soon!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ayush Thakur",
    "section": "",
    "text": "Google Developer Expert Kaggle Notebooks Master\n\n\nHello! I’m a Machine Learning Engineer specializing in LLM applications, model evaluation, and MLOps. My work focuses on building robust ML pipelines, evaluating LLM systems, and developing tools for the open-source community.\nAt Weights & Biases, I develop MLOps solutions for popular repositories like Keras, OpenMMLab, and Meta. I’m also a Google Developer Expert in Machine Learning and a Kaggle Notebooks Master.\nWhen I’m not coding, I enjoy watching anime (Naruto is my all-time favorite!), sharing knowledge through technical articles, and collaborating on interesting ML projects.\n\n\nGitHub | Twitter | LinkedIn | Kaggle | W&B\n\n\nAuthoring Read My Blog View My Projects Talks & Courses\n\n\n\n\n\n\n\n\n\nA comprehensive guide to Retrieval Augmented Generation techniques for enhancing LLMs with external knowledge sources.\n\n\n\nA systematic approach to evaluating LLM performance using W&B Sweeps.\n\n\n\nTensorFlow implementation of self-supervised visual representation learning through contrasting cluster assignments.\n\n\n\n\n\n\n\n\nBuilding, evaluating, and optimizing LLM-based applications.\n\n\n\nImplementing retrieval-augmented generation for knowledge-intensive applications.\n\n\n\nDeveloping robust ML pipelines and infrastructure for continuous delivery.\n\n\n\nImplementing and optimizing visual recognition systems.\n\n\n\nCreating accessible educational content on ML topics.\n\n\n\nSystematically assessing ML model performance across dimensions.\n\n\n\n\n\nI’ve created comprehensive courses on: - RAG++: From POC to Production - Training and Fine-tuning Large Language Models (LLMs)\nExplore All Courses & Talks"
  },
  {
    "objectID": "index.html#machine-learning-engineer-at-weights-biases",
    "href": "index.html#machine-learning-engineer-at-weights-biases",
    "title": "Ayush Thakur",
    "section": "",
    "text": "Google Developer Expert Kaggle Notebooks Master\n\n\nHello! I’m a Machine Learning Engineer specializing in LLM applications, model evaluation, and MLOps. My work focuses on building robust ML pipelines, evaluating LLM systems, and developing tools for the open-source community.\nAt Weights & Biases, I develop MLOps solutions for popular repositories like Keras, OpenMMLab, and Meta. I’m also a Google Developer Expert in Machine Learning and a Kaggle Notebooks Master.\nWhen I’m not coding, I enjoy watching anime (Naruto is my all-time favorite!), sharing knowledge through technical articles, and collaborating on interesting ML projects.\n\n\nGitHub | Twitter | LinkedIn | Kaggle | W&B\n\n\nAuthoring Read My Blog View My Projects Talks & Courses"
  },
  {
    "objectID": "index.html#featured-work",
    "href": "index.html#featured-work",
    "title": "Ayush Thakur",
    "section": "",
    "text": "A comprehensive guide to Retrieval Augmented Generation techniques for enhancing LLMs with external knowledge sources.\n\n\n\nA systematic approach to evaluating LLM performance using W&B Sweeps.\n\n\n\nTensorFlow implementation of self-supervised visual representation learning through contrasting cluster assignments."
  },
  {
    "objectID": "index.html#areas-of-expertise",
    "href": "index.html#areas-of-expertise",
    "title": "Ayush Thakur",
    "section": "",
    "text": "Building, evaluating, and optimizing LLM-based applications.\n\n\n\nImplementing retrieval-augmented generation for knowledge-intensive applications.\n\n\n\nDeveloping robust ML pipelines and infrastructure for continuous delivery.\n\n\n\nImplementing and optimizing visual recognition systems.\n\n\n\nCreating accessible educational content on ML topics.\n\n\n\nSystematically assessing ML model performance across dimensions."
  },
  {
    "objectID": "index.html#courses-education",
    "href": "index.html#courses-education",
    "title": "Ayush Thakur",
    "section": "",
    "text": "I’ve created comprehensive courses on: - RAG++: From POC to Production - Training and Fine-tuning Large Language Models (LLMs)\nExplore All Courses & Talks"
  },
  {
    "objectID": "projects/llm-eval-sweep.html",
    "href": "projects/llm-eval-sweep.html",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "GitHub Repository\n\n\n\n\nThis project demonstrates systematic approaches to evaluating Large Language Models (LLMs) using Weights & Biases Sweeps. It provides practical examples of how to set up comprehensive evaluation pipelines for different types of LLM tasks.\n\n\n\n\nQA Evaluation: Framework for evaluating question-answering performance\nMathematical Reasoning: Specialized evaluation for mathematical problem solving\nParameter Optimization: Using W&B Sweeps to find optimal prompting strategies\nVisualization: Rich visual analytics for interpreting results\n\n\n\n\nThe repository implements several evaluation strategies:\n\n\nOne of the most powerful approaches is using another LLM (typically a more capable one) to evaluate the responses of the target LLM:\ndef evaluate_with_llm_judge(question, reference_answer, model_response):\n    \"\"\"\n    Uses a judge LLM to evaluate the quality of a model response\n    compared to a reference answer.\n    \"\"\"\n    judge_prompt = f\"\"\"\n    Question: {question}\n    Reference Answer: {reference_answer}\n    Model Response: {model_response}\n    \n    Evaluate the model response on the following criteria on a scale of 1-10:\n    1. Accuracy: How factually correct is the response?\n    2. Completeness: How complete is the response?\n    3. Relevance: How relevant is the response to the question?\n    \n    For each criterion, provide a score and a brief explanation.\n    Finally, provide an overall score.\n    \"\"\"\n    \n    # Call the judge LLM\n    evaluation = judge_llm(judge_prompt)\n    \n    # Parse the scores from the evaluation\n    # Implementation depends on the structure of the judge's response\n    \n    return parsed_scores\n\n\n\nThe project uses W&B Sweeps to systematically explore different parameters:\nsweep_config = {\n    \"method\": \"grid\",\n    \"metric\": {\"name\": \"average_accuracy\", \"goal\": \"maximize\"},\n    \"parameters\": {\n        \"temperature\": {\"values\": [0.0, 0.3, 0.7, 1.0]},\n        \"prompt_strategy\": {\"values\": [\"direct\", \"cot\", \"few_shot\"]},\n        \"max_tokens\": {\"values\": [100, 200, 500]},\n    }\n}\nThis allows exploration of how different combinations of temperature, prompting strategies, and token limits affect the model’s performance.\n\n\n\n\nThe repository includes visualizations of sweep results, demonstrating how different parameters affect various performance metrics:\n\nTemperature Impact: Lower temperatures generally improved factual accuracy but sometimes at the cost of completeness\nPrompt Strategies: Chain-of-Thought prompting significantly improved mathematical reasoning tasks\nParameter Interaction: Complex interactions between parameters highlighted the importance of systematic sweeps rather than one-at-a-time optimization\n\n\n\n\nThis evaluation framework can be applied to:\n\nModel Selection: Comparing different LLMs for specific use cases\nPrompt Engineering: Finding optimal prompting strategies\nFine-tuning Decisions: Determining whether fine-tuning would benefit specific tasks\nApplication Development: Building more robust LLM-powered applications\n\n\n\n\nOngoing and planned improvements include:\n\nAdding support for more diverse evaluation tasks\nImplementing automated evaluation for creative and open-ended generation\nIntegrating human feedback into the evaluation pipeline\nExploring adaptive evaluation strategies that adjust to model strengths and weaknesses\n\n\n\n\nTo use this framework for your own LLM evaluation:\n\nClone the repository: git clone https://github.com/ayulockin/llm-eval-sweep.git\nInstall dependencies: pip install -r requirements.txt\nConfigure your W&B credentials\nRun example evaluations: python qa_full_sweeps.py\n\nFor more details, check out the GitHub repository."
  },
  {
    "objectID": "projects/llm-eval-sweep.html#project-overview",
    "href": "projects/llm-eval-sweep.html#project-overview",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "This project demonstrates systematic approaches to evaluating Large Language Models (LLMs) using Weights & Biases Sweeps. It provides practical examples of how to set up comprehensive evaluation pipelines for different types of LLM tasks."
  },
  {
    "objectID": "projects/llm-eval-sweep.html#key-features",
    "href": "projects/llm-eval-sweep.html#key-features",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "QA Evaluation: Framework for evaluating question-answering performance\nMathematical Reasoning: Specialized evaluation for mathematical problem solving\nParameter Optimization: Using W&B Sweeps to find optimal prompting strategies\nVisualization: Rich visual analytics for interpreting results"
  },
  {
    "objectID": "projects/llm-eval-sweep.html#implementation-details",
    "href": "projects/llm-eval-sweep.html#implementation-details",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "The repository implements several evaluation strategies:\n\n\nOne of the most powerful approaches is using another LLM (typically a more capable one) to evaluate the responses of the target LLM:\ndef evaluate_with_llm_judge(question, reference_answer, model_response):\n    \"\"\"\n    Uses a judge LLM to evaluate the quality of a model response\n    compared to a reference answer.\n    \"\"\"\n    judge_prompt = f\"\"\"\n    Question: {question}\n    Reference Answer: {reference_answer}\n    Model Response: {model_response}\n    \n    Evaluate the model response on the following criteria on a scale of 1-10:\n    1. Accuracy: How factually correct is the response?\n    2. Completeness: How complete is the response?\n    3. Relevance: How relevant is the response to the question?\n    \n    For each criterion, provide a score and a brief explanation.\n    Finally, provide an overall score.\n    \"\"\"\n    \n    # Call the judge LLM\n    evaluation = judge_llm(judge_prompt)\n    \n    # Parse the scores from the evaluation\n    # Implementation depends on the structure of the judge's response\n    \n    return parsed_scores\n\n\n\nThe project uses W&B Sweeps to systematically explore different parameters:\nsweep_config = {\n    \"method\": \"grid\",\n    \"metric\": {\"name\": \"average_accuracy\", \"goal\": \"maximize\"},\n    \"parameters\": {\n        \"temperature\": {\"values\": [0.0, 0.3, 0.7, 1.0]},\n        \"prompt_strategy\": {\"values\": [\"direct\", \"cot\", \"few_shot\"]},\n        \"max_tokens\": {\"values\": [100, 200, 500]},\n    }\n}\nThis allows exploration of how different combinations of temperature, prompting strategies, and token limits affect the model’s performance."
  },
  {
    "objectID": "projects/llm-eval-sweep.html#results-and-insights",
    "href": "projects/llm-eval-sweep.html#results-and-insights",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "The repository includes visualizations of sweep results, demonstrating how different parameters affect various performance metrics:\n\nTemperature Impact: Lower temperatures generally improved factual accuracy but sometimes at the cost of completeness\nPrompt Strategies: Chain-of-Thought prompting significantly improved mathematical reasoning tasks\nParameter Interaction: Complex interactions between parameters highlighted the importance of systematic sweeps rather than one-at-a-time optimization"
  },
  {
    "objectID": "projects/llm-eval-sweep.html#practical-applications",
    "href": "projects/llm-eval-sweep.html#practical-applications",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "This evaluation framework can be applied to:\n\nModel Selection: Comparing different LLMs for specific use cases\nPrompt Engineering: Finding optimal prompting strategies\nFine-tuning Decisions: Determining whether fine-tuning would benefit specific tasks\nApplication Development: Building more robust LLM-powered applications"
  },
  {
    "objectID": "projects/llm-eval-sweep.html#future-work",
    "href": "projects/llm-eval-sweep.html#future-work",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "Ongoing and planned improvements include:\n\nAdding support for more diverse evaluation tasks\nImplementing automated evaluation for creative and open-ended generation\nIntegrating human feedback into the evaluation pipeline\nExploring adaptive evaluation strategies that adjust to model strengths and weaknesses"
  },
  {
    "objectID": "projects/llm-eval-sweep.html#get-started",
    "href": "projects/llm-eval-sweep.html#get-started",
    "title": "LLM Evaluation with W&B Sweeps",
    "section": "",
    "text": "To use this framework for your own LLM evaluation:\n\nClone the repository: git clone https://github.com/ayulockin/llm-eval-sweep.git\nInstall dependencies: pip install -r requirements.txt\nConfigure your W&B credentials\nRun example evaluations: python qa_full_sweeps.py\n\nFor more details, check out the GitHub repository."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks & Courses",
    "section": "",
    "text": "This comprehensive course covers everything you need to know about Retrieval Augmented Generation, from basic concepts to production-ready implementations. Learn how to enhance LLMs with external knowledge sources effectively.\nTopics covered: - RAG fundamentals and architecture - Advanced retrieval techniques - Context processing and optimization - Evaluation and monitoring - Deployment considerations\nCourse Link | 2023\n\n\n\nA deep dive into the world of LLM fine-tuning, teaching you how to adapt foundation models to your specific use cases. The course covers various techniques from parameter-efficient methods to full fine-tuning approaches.\nTopics covered: - Understanding LLM architectures - Fine-tuning methods (LoRA, QLoRA, full fine-tuning) - Data preparation and preprocessing - Evaluation and benchmarking - Deployment and serving\nCourse Link | 2023"
  },
  {
    "objectID": "talks.html#courses",
    "href": "talks.html#courses",
    "title": "Talks & Courses",
    "section": "",
    "text": "This comprehensive course covers everything you need to know about Retrieval Augmented Generation, from basic concepts to production-ready implementations. Learn how to enhance LLMs with external knowledge sources effectively.\nTopics covered: - RAG fundamentals and architecture - Advanced retrieval techniques - Context processing and optimization - Evaluation and monitoring - Deployment considerations\nCourse Link | 2023\n\n\n\nA deep dive into the world of LLM fine-tuning, teaching you how to adapt foundation models to your specific use cases. The course covers various techniques from parameter-efficient methods to full fine-tuning approaches.\nTopics covered: - Understanding LLM architectures - Fine-tuning methods (LoRA, QLoRA, full fine-tuning) - Data preparation and preprocessing - Evaluation and benchmarking - Deployment and serving\nCourse Link | 2023"
  },
  {
    "objectID": "talks.html#conference-talks-workshops",
    "href": "talks.html#conference-talks-workshops",
    "title": "Talks & Courses",
    "section": "Conference Talks & Workshops",
    "text": "Conference Talks & Workshops\n\n“Advanced RAG Techniques for Production Systems”\nNeurIPS Workshop | December 2023\nA comprehensive overview of production-grade RAG systems, covering advanced techniques like reranking, query transformation, and modular architectures. The talk included live demonstrations of performance improvements from each technique.\n\n\n“Evaluating and Improving LLM Performance”\nPyData Global | November 2023\nThis workshop provided attendees with a framework for systematically evaluating LLM performance across various dimensions, along with strategies for addressing common weaknesses. Participants learned how to use Weights & Biases to track experiments and improvements.\n\n\n“MLOps for Computer Vision Systems”\nTensorFlow Meetup | May 2022\nAs a Google Developer Expert in TensorFlow, I presented best practices for operationalizing computer vision models, from experiment tracking to deployment. The talk showcased how Weights & Biases can streamline the ML lifecycle."
  },
  {
    "objectID": "talks.html#tutorial-videos",
    "href": "talks.html#tutorial-videos",
    "title": "Talks & Courses",
    "section": "Tutorial Videos",
    "text": "Tutorial Videos\n\n\nUnderstanding Loss Landscapes\nAn exploration of how loss landscapes impact model performance and why ensemble methods work well.\nWatch Video | 25 minutes\n\n\nBuilding a RAG System from Scratch\nStep-by-step tutorial on implementing a basic RAG system using open-source tools.\nWatch Video | 35 minutes\n\n\nFine-tuning LLMs with LoRA\nLearn how to efficiently fine-tune large language models using Parameter-Efficient Fine-Tuning (PEFT) methods.\nWatch Video | 40 minutes"
  },
  {
    "objectID": "talks.html#upcoming-events",
    "href": "talks.html#upcoming-events",
    "title": "Talks & Courses",
    "section": "Upcoming Events",
    "text": "Upcoming Events\n\n“LLM Systems: From Prototype to Production” - ML Summit | June 2024\n“Evaluating Hallucinations in LLMs” - AI Conference | August 2024"
  },
  {
    "objectID": "talks.html#interested-in-having-me-speak",
    "href": "talks.html#interested-in-having-me-speak",
    "title": "Talks & Courses",
    "section": "Interested in Having Me Speak?",
    "text": "Interested in Having Me Speak?\nIf you’re organizing a conference, workshop, or meetup related to machine learning, LLMs, or MLOps, I’d be happy to discuss speaking opportunities. Please contact me with details about your event."
  },
  {
    "objectID": "authoring.html",
    "href": "authoring.html",
    "title": "Authoring",
    "section": "",
    "text": "Below are the blogs, articles, and tutorials I have written on Machine Learning, Deep Learning, and MLOps. I am fortunate to collaborate with many talented individuals globally and share my knowledge with the community.\n\n\n\n\n\nHyperparameter Optimization with Weights & Biases Sweeps\nMultimodal Few-Shot Learning with Frozen Language Models\n\n\n\n\n\nIntroduction to Image Inpainting with Deep Learning (joint work with Sayak Paul)\nUnderstanding the Effectiveness of Ensembles in Deep Learning (joint work with Sayak Paul)\nUnsupervised Visual Representation Learning with SwAV (joint work with Sayak Paul)\nBenchmarking the 🤗 Transformers for Tensorflow 2\nVisualize Class Activation Maps for Image Classification Models\nReproducible Models with W&B\nKeras XLA Benchmarks (in collaboration with Soumik Rakshit and Sayak Paul)\nVisualize Models in TensorBoard with Weights & Biases\nVisualizing Model Performance with W&B\nBuilding LLM Evaluation Frameworks\nMLOps for Computer Vision Systems\n\n\n\n\n\nI regularly publish technical articles on Medium, covering various aspects of machine learning, deep learning, and MLOps:\n\nBuilding Robust LLM Applications\nThe State of LLM Evaluation\nMachine Learning System Design: A Comprehensive Approach\nGradient Amplification Attack: Circumventing Defenses Against Backdoor Attacks\nTraining with Mixed Precision in TensorFlow 2.0\n\n\n\n\nI’ve created in-depth tutorials on various ML/DL topics:\n\nTensorFlow Implementation of SwAV - GitHub repository with detailed notebooks\nLLM Evaluation Strategies - Repository demonstrating systematic LLM evaluation approaches\nE2E MLOps Pipeline on GCP for Tensorflow - End-to-end MLOps on Google Cloud Platform\n\n\n\n\nI enjoy collaborating with other practitioners in the field:\n\nOpen Source Contributions: I build MLOps pipelines for popular repositories like Keras, OpenMMLab, and Meta\nTechnical Reviews: I provide technical reviews for ML/DL content published by various platforms"
  },
  {
    "objectID": "authoring.html#weights-biases",
    "href": "authoring.html#weights-biases",
    "title": "Authoring",
    "section": "",
    "text": "Hyperparameter Optimization with Weights & Biases Sweeps\nMultimodal Few-Shot Learning with Frozen Language Models\n\n\n\n\n\nIntroduction to Image Inpainting with Deep Learning (joint work with Sayak Paul)\nUnderstanding the Effectiveness of Ensembles in Deep Learning (joint work with Sayak Paul)\nUnsupervised Visual Representation Learning with SwAV (joint work with Sayak Paul)\nBenchmarking the 🤗 Transformers for Tensorflow 2\nVisualize Class Activation Maps for Image Classification Models\nReproducible Models with W&B\nKeras XLA Benchmarks (in collaboration with Soumik Rakshit and Sayak Paul)\nVisualize Models in TensorBoard with Weights & Biases\nVisualizing Model Performance with W&B\nBuilding LLM Evaluation Frameworks\nMLOps for Computer Vision Systems"
  },
  {
    "objectID": "authoring.html#medium",
    "href": "authoring.html#medium",
    "title": "Authoring",
    "section": "",
    "text": "I regularly publish technical articles on Medium, covering various aspects of machine learning, deep learning, and MLOps:\n\nBuilding Robust LLM Applications\nThe State of LLM Evaluation\nMachine Learning System Design: A Comprehensive Approach\nGradient Amplification Attack: Circumventing Defenses Against Backdoor Attacks\nTraining with Mixed Precision in TensorFlow 2.0"
  },
  {
    "objectID": "authoring.html#tutorials-guides",
    "href": "authoring.html#tutorials-guides",
    "title": "Authoring",
    "section": "",
    "text": "I’ve created in-depth tutorials on various ML/DL topics:\n\nTensorFlow Implementation of SwAV - GitHub repository with detailed notebooks\nLLM Evaluation Strategies - Repository demonstrating systematic LLM evaluation approaches\nE2E MLOps Pipeline on GCP for Tensorflow - End-to-end MLOps on Google Cloud Platform"
  },
  {
    "objectID": "authoring.html#collaborative-work",
    "href": "authoring.html#collaborative-work",
    "title": "Authoring",
    "section": "",
    "text": "I enjoy collaborating with other practitioners in the field:\n\nOpen Source Contributions: I build MLOps pipelines for popular repositories like Keras, OpenMMLab, and Meta\nTechnical Reviews: I provide technical reviews for ML/DL content published by various platforms"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "TensorFlow implementation of “Unsupervised Learning of Visual Features by Contrasting Cluster Assignments”\nThis project provides a TensorFlow implementation of the SwAV (Swapping Assignments between Views) algorithm for self-supervised visual representation learning. The implementation allows for efficient contrastive learning by contrasting cluster assignments between different views of the same image.\nTechnologies: TensorFlow, Contrastive Learning, Computer Vision\n\n\n\nExploring the ideas from “Deep Ensembles: A Loss Landscape Perspective”\nThis repository explores the concepts presented in the paper “Deep Ensembles: A Loss Landscape Perspective.” It visualizes loss landscapes to understand why deep ensembles work better than other methods for uncertainty estimation and how they relate to the geometry of loss functions.\nTechnologies: TensorFlow, Deep Learning, Uncertainty Estimation\n\n\n\nStarter pack for NeurIPS LLM Efficiency Challenge 2023\nThis repository provides a starter kit for participants in the NeurIPS LLM Efficiency Challenge 2023. It includes code and resources to help competitors optimize LLMs for better efficiency while maintaining performance.\nTechnologies: PyTorch, LLMs, Model Optimization\n\n\n\nDeep Image Inpainting using UNET-like Vanilla Autoencoder and Partial Convolution based Autoencoder\nThis project implements image inpainting techniques using two different approaches: a UNET-like Vanilla Autoencoder and a Partial Convolution based Autoencoder. It enables filling in missing or corrupted parts of images with plausible content.\nTechnologies: PyTorch, Computer Vision, Image Generation\n\n\n\nLLM Evaluation Strategies with W&B Sweeps\nA repository showcasing various LLM evaluation strategies and leveraging Weights & Biases Sweeps to optimize LLM systems. It provides practical demonstrations of how to evaluate and improve LLM performance systematically.\nTechnologies: PyTorch, Weights & Biases, LLMs, Evaluation\n\n\n\nComprehensive Article on RAG: From naive to advanced\nAn in-depth article exploring Retrieval Augmented Generation (RAG) techniques, covering basic implementations to advanced strategies. This resource helps developers understand how to enhance LLMs with external knowledge effectively.\nTechnologies: LLMs, RAG, Information Retrieval"
  },
  {
    "objectID": "projects.html#featured-projects",
    "href": "projects.html#featured-projects",
    "title": "Projects",
    "section": "",
    "text": "TensorFlow implementation of “Unsupervised Learning of Visual Features by Contrasting Cluster Assignments”\nThis project provides a TensorFlow implementation of the SwAV (Swapping Assignments between Views) algorithm for self-supervised visual representation learning. The implementation allows for efficient contrastive learning by contrasting cluster assignments between different views of the same image.\nTechnologies: TensorFlow, Contrastive Learning, Computer Vision\n\n\n\nExploring the ideas from “Deep Ensembles: A Loss Landscape Perspective”\nThis repository explores the concepts presented in the paper “Deep Ensembles: A Loss Landscape Perspective.” It visualizes loss landscapes to understand why deep ensembles work better than other methods for uncertainty estimation and how they relate to the geometry of loss functions.\nTechnologies: TensorFlow, Deep Learning, Uncertainty Estimation\n\n\n\nStarter pack for NeurIPS LLM Efficiency Challenge 2023\nThis repository provides a starter kit for participants in the NeurIPS LLM Efficiency Challenge 2023. It includes code and resources to help competitors optimize LLMs for better efficiency while maintaining performance.\nTechnologies: PyTorch, LLMs, Model Optimization\n\n\n\nDeep Image Inpainting using UNET-like Vanilla Autoencoder and Partial Convolution based Autoencoder\nThis project implements image inpainting techniques using two different approaches: a UNET-like Vanilla Autoencoder and a Partial Convolution based Autoencoder. It enables filling in missing or corrupted parts of images with plausible content.\nTechnologies: PyTorch, Computer Vision, Image Generation\n\n\n\nLLM Evaluation Strategies with W&B Sweeps\nA repository showcasing various LLM evaluation strategies and leveraging Weights & Biases Sweeps to optimize LLM systems. It provides practical demonstrations of how to evaluate and improve LLM performance systematically.\nTechnologies: PyTorch, Weights & Biases, LLMs, Evaluation\n\n\n\nComprehensive Article on RAG: From naive to advanced\nAn in-depth article exploring Retrieval Augmented Generation (RAG) techniques, covering basic implementations to advanced strategies. This resource helps developers understand how to enhance LLMs with external knowledge effectively.\nTechnologies: LLMs, RAG, Information Retrieval"
  },
  {
    "objectID": "projects.html#open-source-contributions",
    "href": "projects.html#open-source-contributions",
    "title": "Projects",
    "section": "Open Source Contributions",
    "text": "Open Source Contributions\nI actively contribute to various open-source projects, focusing on machine learning tools and libraries. Some of my contributions include:\n\nKeras: Implementing MLOps pipelines and enhancements\nOpenMMLab repositories: Building and optimizing workflows\nMeta repositories: Contributing to machine learning infrastructure"
  },
  {
    "objectID": "projects.html#looking-to-collaborate",
    "href": "projects.html#looking-to-collaborate",
    "title": "Projects",
    "section": "Looking to Collaborate?",
    "text": "Looking to Collaborate?\nI’m always interested in collaborating on machine learning projects, especially in computer vision (except face detection). If you have an interesting project idea or want to collaborate, feel free to reach out to me on Twitter or GitHub."
  },
  {
    "objectID": "blog/posts/llm-evaluation.html",
    "href": "blog/posts/llm-evaluation.html",
    "title": "The State of LLM Evaluation",
    "section": "",
    "text": "The State of LLM Evaluation\nContent coming soon!\nCheck out my repository for more information on LLM evaluation approaches: LLM Evaluation Strategies"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog where I share my thoughts, experiences, and tutorials on machine learning, LLMs, and MLOps. I cover topics ranging from technical deep dives to practical implementation guides, with a focus on making complex concepts accessible.\nIf you’re interested in specific topics, use the filters above to narrow down the posts by category, or use the search function to find exactly what you’re looking for.\nFeel free to share these posts and reach out with questions or feedback. I’m always happy to engage with the community!\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nThe State of LLM Evaluation\n\n\n\n\n\n\nllm\n\n\nevaluation\n\n\nmachine-learning\n\n\n\nAn overview of current approaches and challenges in LLM evaluation\n\n\n\n\n\nDec 12, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nRAG Techniques: From Naive to Advanced\n\n\n\n\n\n\nllm\n\n\nrag\n\n\ntechniques\n\n\n\nAn overview of Retrieval Augmented Generation (RAG) techniques for enhancing LLM responses with external knowledge\n\n\n\n\n\nNov 28, 2023\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  }
]